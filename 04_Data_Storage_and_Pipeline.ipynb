{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2788ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Notebook 4: Data Storage and Pipeline ---\n",
      "Current time: 2025-05-23 09:45:14.831961+05:30\n",
      "Using Database: trend_analyzer.db\n",
      "\n",
      "All necessary functions and configurations loaded/redefined for Notebook 4 pipeline.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime, timezone, date # Added date\n",
    "import time\n",
    "import pandas as pd # For potential display, though not strictly needed for pipeline logic\n",
    "\n",
    "# --- Essential Config variables (copied/adapted from Notebook 01) ---\n",
    "DATABASE_NAME = \"trend_analyzer.db\"\n",
    "NEWS_RSS_FEEDS = { # Use your working feeds\n",
    "    \"Guardian Environment\": \"https://www.theguardian.com/environment/rss\",\n",
    "    \"Ars Technica\": \"http://feeds.arstechnica.com/arstechnica/index/\" # Example, replace with relevant ones\n",
    "}\n",
    "REDDIT_SUBREDDITS = {\n",
    "    \"RenewableEnergySub\": \"RenewableEnergy\"\n",
    "}\n",
    "REDDIT_POST_LIMIT = 5 # Keep small for pipeline testing in notebook\n",
    "\n",
    "print(f\"--- Notebook 4: Data Storage and Pipeline ---\")\n",
    "print(f\"Current time: {pd.Timestamp.now(tz='Asia/Kolkata')}\")\n",
    "print(f\"Using Database: {DATABASE_NAME}\")\n",
    "\n",
    "# --- Database Manager Functions (copied/adapted from Notebook 01 & previous cells) ---\n",
    "def create_connection():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(DATABASE_NAME)\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"DB Connection Error: {e}\")\n",
    "    return conn\n",
    "\n",
    "def insert_article_data_pipeline(conn, article_list):\n",
    "    \"\"\"Inserts new articles and returns a map of {url: new_article_id} for successfully inserted articles.\"\"\"\n",
    "    inserted_articles_map = {} # url -> new_id\n",
    "    if not conn or not article_list:\n",
    "        return inserted_articles_map\n",
    "\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        # Get existing URLs to avoid duplicates\n",
    "        cursor.execute(\"SELECT source_url FROM articles\")\n",
    "        existing_urls = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "        for article in article_list:\n",
    "            if article['url'] not in existing_urls:\n",
    "                try:\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO articles (source_url, source_name, title, raw_content, publication_date)\n",
    "                        VALUES (?, ?, ?, ?, ?)\n",
    "                    \"\"\", (article['url'], article['source_name'], article['title'],\n",
    "                          article['full_content'], article.get('publication_date')))\n",
    "                    inserted_articles_map[article['url']] = cursor.lastrowid\n",
    "                except sqlite3.IntegrityError:\n",
    "                    # This case should ideally be prevented by the 'not in existing_urls' check\n",
    "                    # print(f\"IntegrityError (should be rare): Article URL {article['url']} already exists.\")\n",
    "                    pass # Skip if somehow it still happens\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting article {article['url']}: {e}\")\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"DB error during bulk article insertion: {e}\")\n",
    "    return inserted_articles_map\n",
    "\n",
    "def insert_nlp_data_pipeline(conn, article_id, sentiment, keywords, entities):\n",
    "    \"\"\"Inserts NLP results for a given article_id.\"\"\"\n",
    "    if not conn:\n",
    "        return False\n",
    "    success = False\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        # Insert sentiment\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO sentiments (article_id, sentiment_score, sentiment_label)\n",
    "            VALUES (?, ?, ?)\n",
    "        \"\"\", (article_id, sentiment['score'], sentiment['label']))\n",
    "\n",
    "        # Insert keywords\n",
    "        for keyword, score in keywords.items(): # keywords is a dict\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO keywords (article_id, keyword, score) VALUES (?, ?, ?)\n",
    "            \"\"\", (article_id, keyword, score))\n",
    "\n",
    "        # Insert entities\n",
    "        for entity_text, entity_label in entities.items(): # entities is a dict\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO entities (article_id, entity_text, entity_label) VALUES (?, ?, ?)\n",
    "            \"\"\", (article_id, entity_text, entity_label))\n",
    "        conn.commit()\n",
    "        success = True\n",
    "    except sqlite3.Error as e:\n",
    "        # print(f\"Error inserting NLP data for article_id {article_id}: {e}\")\n",
    "        # Allow pipeline to continue for other articles if one fails NLP insertion\n",
    "        pass\n",
    "    return success\n",
    "\n",
    "# --- Scraper Functions (copied/adapted from Notebook 02) ---\n",
    "import requests\n",
    "import feedparser\n",
    "from newspaper import Article as NewspaperArticle, ArticleException\n",
    "\n",
    "def parse_datetime(date_string): # Copied from Notebook 2, Cell 2\n",
    "    if not date_string: return None\n",
    "    common_formats = ['%a, %d %b %Y %H:%M:%S %z', '%a, %d %b %Y %H:%M:%S %Z', '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%dT%H:%M:%S.%f%z', '%Y-%m-%d %H:%M:%S']\n",
    "    dt_object = None\n",
    "    if isinstance(date_string, (int, float)):\n",
    "        try: return datetime.fromtimestamp(date_string, timezone.utc)\n",
    "        except: pass\n",
    "    if isinstance(date_string, str):\n",
    "        for fmt in common_formats:\n",
    "            try:\n",
    "                dt_object = datetime.strptime(date_string, fmt)\n",
    "                if dt_object.tzinfo is None or dt_object.tzinfo.utcoffset(dt_object) is None: dt_object = dt_object.replace(tzinfo=timezone.utc)\n",
    "                return dt_object\n",
    "            except ValueError: continue\n",
    "    elif hasattr(date_string, 'tm_year'): return datetime.fromtimestamp(time.mktime(date_string), timezone.utc)\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "def fetch_article_content(url): # Copied from Notebook 2, Cell 3 (corrected)\n",
    "    if not url or not url.startswith(('http://', 'https://')): return None, None\n",
    "    try:\n",
    "        article_obj = NewspaperArticle(url, fetch_images=False, memoize_articles=False)\n",
    "        article_obj.download(); article_obj.parse()\n",
    "        text = article_obj.text; pub_date = article_obj.publish_date\n",
    "        if pub_date and (pub_date.tzinfo is None or pub_date.tzinfo.utcoffset(pub_date) is None): pub_date = pub_date.replace(tzinfo=timezone.utc)\n",
    "        return text, pub_date\n",
    "    except: return None, None # Simplified error handling for brevity here\n",
    "\n",
    "def scrape_rss_feed_pipeline(source_name, rss_url): # Adapted from Notebook 2, Cell 4 (corrected)\n",
    "    articles = []; feed_data = None\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; TrendAnalyzerBot-Pipeline/0.1)', 'Accept': 'application/xml,text/xml,*/*'}\n",
    "    try:\n",
    "        response = requests.get(rss_url, headers=headers, timeout=15); response.raise_for_status()\n",
    "        feed_content = response.content; feed_data = feedparser.parse(feed_content)\n",
    "        entries_to_process = feed_data.entries[:REDDIT_POST_LIMIT] # Using global limit for consistency\n",
    "        for entry in entries_to_process:\n",
    "            title = entry.get('title'); url = entry.get('link')\n",
    "            pub_date_parsed = entry.get('published_parsed') or entry.get('updated_parsed')\n",
    "            pub_date_feed = parse_datetime(pub_date_parsed) if pub_date_parsed else None\n",
    "            if not (title and url): continue\n",
    "            full_content, pub_date_article = fetch_article_content(url)\n",
    "            final_pub_date = pub_date_feed if pub_date_feed else pub_date_article if pub_date_article else datetime.now(timezone.utc)\n",
    "            content_to_store = full_content if full_content and full_content.strip() else f\"Content not retrievable - {title}\"\n",
    "            articles.append({'title': title, 'url': url, 'publication_date': final_pub_date, 'full_content': content_to_store, 'source_name': source_name})\n",
    "            time.sleep(0.1) # Be polite\n",
    "    except Exception as e: print(f\"Error scraping RSS {source_name} ({rss_url}): {e}\")\n",
    "    return articles\n",
    "\n",
    "def scrape_reddit_forum_pipeline(source_name_prefix, subreddit_name, limit): # Adapted from Notebook 2, Cell 5\n",
    "    articles = []; headers = {'User-agent': f'Mozilla/5.0 (compatible; {source_name_prefix}_TrendAnalyzerBot-Pipeline/0.1)'}\n",
    "    try:\n",
    "        url = f\"https://www.reddit.com/r/{subreddit_name}/new.json?limit={limit}\"; response = requests.get(url, headers=headers, timeout=15); response.raise_for_status(); data = response.json()\n",
    "        if 'data' not in data or 'children' not in data['data']: return articles\n",
    "        for post in data['data']['children']:\n",
    "            post_data = post['data']; title = post_data.get('title'); permalink = post_data.get('permalink')\n",
    "            if not (title and permalink): continue\n",
    "            full_url = f\"https://www.reddit.com{permalink}\"; created_utc = post_data.get('created_utc'); pub_date = parse_datetime(created_utc)\n",
    "            content = \"\"; source_display_name = f\"{source_name_prefix} r/{subreddit_name}\"\n",
    "            if post_data.get('is_self', False): content = post_data.get('selftext', '')\n",
    "            elif 'url_overridden_by_dest' in post_data:\n",
    "                external_url = post_data['url_overridden_by_dest']; fetched_content, _ = fetch_article_content(external_url)\n",
    "                content = fetched_content if fetched_content else title\n",
    "            else: content = title\n",
    "            content_to_store = content if content and content.strip() else f\"Content not retrievable or empty - {title}\"\n",
    "            articles.append({'title': title, 'url': full_url, 'publication_date': pub_date, 'full_content': content_to_store, 'source_name': source_display_name})\n",
    "            time.sleep(0.2) # Be polite\n",
    "    except Exception as e: print(f\"Error scraping Reddit r/{subreddit_name}: {e}\")\n",
    "    return articles\n",
    "\n",
    "# --- NLP Processor Functions (copied/adapted from Notebook 03) ---\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "try: nlp_spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "except OSError: nlp_spacy_model = None; print(\"spaCy model 'en_core_web_sm' not loaded for pipeline.\")\n",
    "try: vader_model = SentimentIntensityAnalyzer()\n",
    "except Exception: vader_model = None; print(\"VADER model not initialized for pipeline.\")\n",
    "\n",
    "def get_sentiment_pipeline(text):\n",
    "    if not text or not isinstance(text, str) or not vader_model: return {'score': 0.0, 'label': 'neutral'}\n",
    "    vs = vader_model.polarity_scores(text); score = vs['compound']\n",
    "    if score >= 0.05: label = 'positive'\n",
    "    elif score <= -0.05: label = 'negative'\n",
    "    else: label = 'neutral'\n",
    "    return {'score': score, 'label': label}\n",
    "\n",
    "def get_entities_pipeline(text):\n",
    "    if not text or not isinstance(text, str) or not nlp_spacy_model: return {}\n",
    "    doc = nlp_spacy_model(text[:nlp_spacy_model.max_length]); entities = {}\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents: entities[ent.text.strip()] = ent.label_\n",
    "    return entities\n",
    "\n",
    "def get_keywords_tfidf_pipeline(text_list, num_keywords=10):\n",
    "    if not text_list or not all(isinstance(s, str) and s.strip() for s in text_list if s): return {}\n",
    "    valid_texts = [s for s in text_list if s and s.strip()];\n",
    "    if not valid_texts: return {}\n",
    "    try:\n",
    "        stop_words = list(nlp_spacy_model.Defaults.stop_words) if nlp_spacy_model else 'english'\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=1000, ngram_range=(1,2), token_pattern=r'(?u)\\b\\w[\\w-]*\\w\\b|\\b\\w\\w+\\b')\n",
    "        tfidf_matrix = vectorizer.fit_transform(valid_texts); feature_names = vectorizer.get_feature_names_out()\n",
    "        doc_vector = tfidf_matrix[0]; tfidf_scores_dict = {}\n",
    "        for col_idx in doc_vector.nonzero()[1]: tfidf_scores_dict[feature_names[col_idx]] = doc_vector[0, col_idx]\n",
    "        return dict(sorted(tfidf_scores_dict.items(), key=lambda item: item[1], reverse=True)[:num_keywords])\n",
    "    except: return {} # Simplified error handling\n",
    "\n",
    "def process_text_content_pipeline(text_content):\n",
    "    if not text_content or not isinstance(text_content, str) or not text_content.strip():\n",
    "        return {'score': 0.0, 'label': 'neutral'}, {}, {}\n",
    "    if not nlp_spacy_model or not vader_model:\n",
    "        # print(\"NLP models not available in process_text_content_pipeline\")\n",
    "        return {'score': 0.0, 'label': 'neutral'}, {}, {}\n",
    "    sentiment = get_sentiment_pipeline(text_content)\n",
    "    entities = get_entities_pipeline(text_content)\n",
    "    keywords = get_keywords_tfidf_pipeline([text_content])\n",
    "    return sentiment, keywords, entities\n",
    "\n",
    "print(\"\\nAll necessary functions and configurations loaded/redefined for Notebook 4 pipeline.\")\n",
    "if not nlp_spacy_model or not vader_model:\n",
    "    print(\"WARNING: One or both NLP models (spaCy, VADER) did not load. NLP processing will be skipped or return defaults.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285fa9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting full data pipeline at 2025-05-23 09:46:42.999598+05:30 ---\n",
      "\n",
      "--- Step 1: Scraping Data ---\n",
      "  Scraping RSS: Guardian Environment...\n",
      "  Scraping RSS: Ars Technica...\n",
      "  Scraping Reddit: r/RenewableEnergy...\n",
      "Total 15 articles/posts scraped from all sources.\n",
      "\n",
      "--- Step 2: Inserting New Articles into Database ---\n",
      "No new unique articles were inserted into the database during this run.\n",
      "\n",
      "--- Step 3: Processing NLP and Storing Results for New Articles ---\n",
      "No new articles to process with NLP.\n",
      "\n",
      "--- Step 4: Daily Trends Calculation ---\n",
      "To calculate/update daily trends based on all data (including newly added),\n",
      "please proceed to and run Notebook 05_Analytics_Engine.ipynb.\n",
      "\n",
      "--- Full data pipeline run (Scraping, NLP for new, Storing) finished in 18.87 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def run_full_pipeline_notebook():\n",
    "    print(f\"\\n--- Starting full data pipeline at {pd.Timestamp.now(tz='Asia/Kolkata')} ---\")\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    # Ensure database tables exist (idempotent check)\n",
    "    # In a script, you might do this once at the beginning. In a notebook, it's fine to re-check.\n",
    "    # create_tables() # This function is from Notebook 1, not redefined in Cell 1 of this notebook.\n",
    "    # For this notebook to be self-contained for pipeline run, let's ensure tables if not done.\n",
    "    # We'll rely on Notebook 1 having been run for table creation. If not, an error will occur on insert.\n",
    "    # Alternatively, copy create_tables() into Cell 1 of this notebook too.\n",
    "    # For now, assuming tables exist from Notebook 1.\n",
    "\n",
    "    # 1. Scrape data\n",
    "    print(\"\\n--- Step 1: Scraping Data ---\")\n",
    "    scraped_articles_list = []\n",
    "    if NEWS_RSS_FEEDS:\n",
    "        for name, url in NEWS_RSS_FEEDS.items():\n",
    "            print(f\"  Scraping RSS: {name}...\")\n",
    "            scraped_articles_list.extend(scrape_rss_feed_pipeline(name, url))\n",
    "    if REDDIT_SUBREDDITS:\n",
    "        for key, sub_name in REDDIT_SUBREDDITS.items():\n",
    "            print(f\"  Scraping Reddit: r/{sub_name}...\")\n",
    "            scraped_articles_list.extend(scrape_reddit_forum_pipeline(key, sub_name, limit=REDDIT_POST_LIMIT))\n",
    "    \n",
    "    if not scraped_articles_list:\n",
    "        print(\"No articles/posts scraped in this run. Pipeline ending.\")\n",
    "        return\n",
    "    print(f\"Total {len(scraped_articles_list)} articles/posts scraped from all sources.\")\n",
    "    # display(pd.DataFrame(scraped_articles_list).head(2)) # Optional: display a few scraped items\n",
    "\n",
    "    # 2. Insert new articles into database and get their IDs\n",
    "    print(\"\\n--- Step 2: Inserting New Articles into Database ---\")\n",
    "    db_conn_pipeline = create_connection()\n",
    "    if not db_conn_pipeline:\n",
    "        print(\"Failed to connect to database. Cannot proceed with article insertion.\")\n",
    "        return\n",
    "        \n",
    "    # insert_article_data_pipeline now handles checking for existing URLs internally\n",
    "    newly_inserted_articles_map = insert_article_data_pipeline(db_conn_pipeline, scraped_articles_list)\n",
    "    # No need to close connection here if insert_article_data_pipeline doesn't close it.\n",
    "    # However, the version I provided for insert_article_data_pipeline in Cell 1 *does* close it.\n",
    "    # So, we need to reopen for NLP data insertion, or modify insert_article_data_pipeline.\n",
    "    # Let's modify for clarity: insert_article_data_pipeline will keep connection open IF passed.\n",
    "    \n",
    "    # Re-architecting slightly for better connection management:\n",
    "    # The insert_article_data_pipeline and insert_nlp_data_pipeline in Cell 1\n",
    "    # already manage their own connections. This is fine for simplicity here.\n",
    "\n",
    "    if not newly_inserted_articles_map:\n",
    "        print(\"No new unique articles were inserted into the database during this run.\")\n",
    "        # We might still want to run analytics for the day, so don't necessarily exit pipeline.\n",
    "    else:\n",
    "        print(f\"{len(newly_inserted_articles_map)} new unique articles were inserted into the database.\")\n",
    "\n",
    "    # 3. Process with NLP and insert NLP data for newly inserted articles\n",
    "    print(\"\\n--- Step 3: Processing NLP and Storing Results for New Articles ---\")\n",
    "    processed_nlp_count = 0\n",
    "    if newly_inserted_articles_map and (nlp_spacy_model and vader_model): # Check if NLP models are loaded\n",
    "        # We need the original article content for those that were newly inserted.\n",
    "        # Iterate through the original scraped list, and if its URL is in our map of new ones, process it.\n",
    "        for original_article in scraped_articles_list:\n",
    "            article_url = original_article['url']\n",
    "            if article_url in newly_inserted_articles_map:\n",
    "                new_article_id = newly_inserted_articles_map[article_url]\n",
    "                content_to_process = original_article.get('full_content', '')\n",
    "                \n",
    "                # print(f\"  NLP for new Article ID: {new_article_id} - Title: {original_article['title'][:30]}...\")\n",
    "                if content_to_process and content_to_process.strip():\n",
    "                    sentiment, keywords, entities = process_text_content_pipeline(content_to_process)\n",
    "                    \n",
    "                    # Re-open connection for this NLP insert batch\n",
    "                    nlp_db_conn = create_connection()\n",
    "                    if nlp_db_conn:\n",
    "                        if insert_nlp_data_pipeline(nlp_db_conn, new_article_id, sentiment, keywords, entities):\n",
    "                            processed_nlp_count += 1\n",
    "                        nlp_db_conn.close() # Close after this batch of NLP inserts for the article\n",
    "                    else:\n",
    "                        print(f\"Failed to connect to DB for NLP insert (Article ID: {new_article_id}). Skipping NLP storage for this item.\")\n",
    "                else:\n",
    "                    # print(f\"  Skipping NLP for Article ID {new_article_id} due to empty/invalid content.\")\n",
    "                    pass\n",
    "        print(f\"NLP processing and storage completed for {processed_nlp_count} new articles.\")\n",
    "    elif not newly_inserted_articles_map:\n",
    "        print(\"No new articles to process with NLP.\")\n",
    "    else:\n",
    "        print(\"NLP models (spaCy/VADER) not loaded. Skipping NLP processing step.\")\n",
    "\n",
    "    # 4. Calculate daily trends (This will be done in Notebook 5)\n",
    "    print(\"\\n--- Step 4: Daily Trends Calculation ---\")\n",
    "    print(\"To calculate/update daily trends based on all data (including newly added),\")\n",
    "    print(\"please proceed to and run Notebook 05_Analytics_Engine.ipynb.\")\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    print(f\"\\n--- Full data pipeline run (Scraping, NLP for new, Storing) finished in {end_time_total - start_time_total:.2f} seconds ---\")\n",
    "\n",
    "# --- Execute the Full Pipeline ---\n",
    "# Before running, ensure your NEWS_RSS_FEEDS in Cell 1 of this notebook\n",
    "# contains valid and relevant URLs that worked in your Notebook 2 tests.\n",
    "if __name__ == '__main__' and '__file__' not in globals(): # Standard check to run code in a notebook cell\n",
    "    # Ensure tables exist (call the function from Notebook 1 if you haven't)\n",
    "    # For this notebook, let's assume Notebook 1's Cell 4 (create_tables) has been run.\n",
    "    # If you want to be absolutely sure tables exist before this pipeline, you'd add:\n",
    "    # import database_manager # (if you made it a .py file)\n",
    "    # database_manager.create_tables()\n",
    "    # OR copy the create_tables function into Cell 1 of this notebook and call it.\n",
    "    # For now, we proceed assuming tables are ready.\n",
    "    \n",
    "    run_full_pipeline_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36909952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
