{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57b3539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP libraries imported.\n",
      "Current time: 2025-05-23 09:17:32.196486+05:30\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter # Though not directly used in the provided functions, it's good for keyword counting if you extend\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string # For punctuation, though spaCy handles a lot of this\n",
    "import pandas as pd # For creating and displaying DataFrames with NLP results\n",
    "\n",
    "print(\"NLP libraries imported.\")\n",
    "print(f\"Current time: {pd.Timestamp.now(tz='Asia/Kolkata')}\") # Using pandas for timezone-aware current time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46371922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model 'en_core_web_sm' loaded successfully.\n",
      "VADER sentiment analyzer initialized successfully.\n",
      "Both spaCy and VADER are ready.\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "# This assumes you have already run: python -m spacy download en_core_web_sm\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model 'en_core_web_sm' loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"spaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run the following in your terminal (with your virtual environment activated):\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    print(\"Then, restart this Jupyter kernel and re-run this cell.\")\n",
    "    nlp = None # Set to None if loading fails\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "try:\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    print(\"VADER sentiment analyzer initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing VADER: {e}\")\n",
    "    vader_analyzer = None\n",
    "\n",
    "# Quick check\n",
    "if nlp and vader_analyzer:\n",
    "    print(\"Both spaCy and VADER are ready.\")\n",
    "else:\n",
    "    print(\"One or more NLP tools could not be initialized. Please check messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ef1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_sentiment function defined.\n",
      "\n",
      "--- Testing get_sentiment function ---\n",
      "Sentence 1: 'This is a great and wonderful development for renewable energy!'\n",
      "  Sentiment: Score=0.880, Label='positive'\n",
      "Sentence 2: 'The progress is terribly slow and disappointing for the sector.'\n",
      "  Sentiment: Score=-0.612, Label='negative'\n",
      "Sentence 3: 'The report was published today.'\n",
      "  Sentiment: Score=0.000, Label='neutral'\n",
      "Sentence 4: 'Solar power is good, but wind energy can sometimes be unreliable.'\n",
      "  Sentiment: Score=0.557, Label='positive'\n",
      "Sentence 5: ''\n",
      "  Sentiment: Score=0.000, Label='neutral'\n",
      "Sentence 6: 'None'\n",
      "  Sentiment: Score=0.000, Label='neutral'\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of a given text using VADER.\n",
    "    Returns a dictionary with a compound score and a label ('positive', 'negative', 'neutral').\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str) or not vader_analyzer: # Added check for vader_analyzer\n",
    "        return {'score': 0.0, 'label': 'neutral'} # Default for empty text or if VADER isn't loaded\n",
    "        \n",
    "    vs = vader_analyzer.polarity_scores(text)\n",
    "    score = vs['compound'] # The compound score is a normalized, weighted composite score.\n",
    "    \n",
    "    # Determine label based on compound score thresholds\n",
    "    if score >= 0.05:\n",
    "        label = 'positive'\n",
    "    elif score <= -0.05:\n",
    "        label = 'negative'\n",
    "    else:\n",
    "        label = 'neutral'\n",
    "        \n",
    "    return {'score': score, 'label': label}\n",
    "\n",
    "print(\"get_sentiment function defined.\")\n",
    "\n",
    "# --- Test cases for get_sentiment ---\n",
    "print(\"\\n--- Testing get_sentiment function ---\")\n",
    "test_sentiments = [\n",
    "    \"This is a great and wonderful development for renewable energy!\", # Expected positive\n",
    "    \"The progress is terribly slow and disappointing for the sector.\",   # Expected negative\n",
    "    \"The report was published today.\",                                  # Expected neutral\n",
    "    \"Solar power is good, but wind energy can sometimes be unreliable.\", # Mixed, VADER will give a compound\n",
    "    \"\", # Empty string\n",
    "    None # None input\n",
    "]\n",
    "\n",
    "if vader_analyzer: # Only run tests if VADER is available\n",
    "    for i, sentence in enumerate(test_sentiments):\n",
    "        sentiment_result = get_sentiment(sentence)\n",
    "        print(f\"Sentence {i+1}: '{sentence}'\")\n",
    "        print(f\"  Sentiment: Score={sentiment_result['score']:.3f}, Label='{sentiment_result['label']}'\")\n",
    "else:\n",
    "    print(\"VADER analyzer not initialized. Skipping get_sentiment tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751ac335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_entities function defined.\n",
      "\n",
      "--- Testing get_entities function ---\n",
      "Sentence 1: 'Apple Inc. is looking at buying U.K. startup for $1 billion in London.'\n",
      "  Entities: {'Apple Inc.': 'ORG', 'U.K.': 'GPE', '$1 billion': 'MONEY', 'London': 'GPE'}\n",
      "Sentence 2: 'Dr. Emily Carter from Princeton University published a paper on solar energy in Germany.'\n",
      "  Entities: {'Emily Carter': 'PERSON', 'Princeton University': 'ORG', 'Germany': 'GPE'}\n",
      "Sentence 3: 'The recent G7 summit discussed advancements in renewable technology in Tokyo last week.'\n",
      "  Entities: {'G7': 'PRODUCT', 'Tokyo': 'GPE', 'last week': 'DATE'}\n",
      "Sentence 4: 'Tesla and SpaceX, companies led by Elon Musk, are pushing boundaries.'\n",
      "  Entities: {'Tesla': 'ORG', 'Elon Musk': 'PERSON'}\n",
      "Sentence 5: ''\n",
      "  Entities: {}\n"
     ]
    }
   ],
   "source": [
    "def get_entities(text):\n",
    "    \"\"\"\n",
    "    Extracts named entities from a given text using spaCy.\n",
    "    Returns a dictionary where keys are entity texts and values are their labels.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str) or not nlp: # Added check for nlp\n",
    "        return {} # Default for empty text or if spaCy isn't loaded\n",
    "        \n",
    "    # Process the text with spaCy\n",
    "    # spaCy's default models have a length limit (e.g., 1,000,000 characters for en_core_web_sm)\n",
    "    # Truncate text if it's too long to prevent errors, though very long single \"texts\" from articles are unlikely.\n",
    "    doc = nlp(text[:nlp.max_length]) \n",
    "    \n",
    "    entities = {}\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            # Store entity text and its label\n",
    "            # Using ent.text.strip() to remove leading/trailing whitespace from entity text\n",
    "            entities[ent.text.strip()] = ent.label_\n",
    "            \n",
    "    return entities\n",
    "\n",
    "print(\"get_entities function defined.\")\n",
    "\n",
    "# --- Test cases for get_entities ---\n",
    "print(\"\\n--- Testing get_entities function ---\")\n",
    "test_entities_text = [\n",
    "    \"Apple Inc. is looking at buying U.K. startup for $1 billion in London.\",\n",
    "    \"Dr. Emily Carter from Princeton University published a paper on solar energy in Germany.\",\n",
    "    \"The recent G7 summit discussed advancements in renewable technology in Tokyo last week.\",\n",
    "    \"Tesla and SpaceX, companies led by Elon Musk, are pushing boundaries.\",\n",
    "    \"\" # Empty string\n",
    "]\n",
    "\n",
    "if nlp: # Only run tests if spaCy is available\n",
    "    for i, sentence in enumerate(test_entities_text):\n",
    "        entities_result = get_entities(sentence)\n",
    "        print(f\"Sentence {i+1}: '{sentence}'\")\n",
    "        print(f\"  Entities: {entities_result}\")\n",
    "else:\n",
    "    print(\"spaCy nlp model not initialized. Skipping get_entities tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a451db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_keywords_tfidf function defined.\n",
      "\n",
      "--- Testing get_keywords_tfidf function ---\n",
      "Text 1: 'Renewable energy sources like solar power and wind turbines are crucial for combating climate change. Investment in green technology is increasing.'\n",
      "  Keywords (TF-IDF): {'technology increasing': 0.1796053020267749, 'green technology': 0.1796053020267749, 'investment green': 0.1796053020267749, 'change investment': 0.1796053020267749, 'climate change': 0.1796053020267749, 'combating climate': 0.1796053020267749, 'crucial combating': 0.1796053020267749, 'turbines crucial': 0.1796053020267749, 'wind turbines': 0.1796053020267749, 'power wind': 0.1796053020267749}\n",
      "Text 2: 'The report details financial spending and budget allocations for the upcoming fiscal year. It's a very dry document about economics.'\n",
      "  Keywords (TF-IDF): {'document economics': 0.20851441405707477, 'dry document': 0.20851441405707477, 'year dry': 0.20851441405707477, 'fiscal year': 0.20851441405707477, 'upcoming fiscal': 0.20851441405707477, 'allocations upcoming': 0.20851441405707477, 'budget allocations': 0.20851441405707477, 'spending budget': 0.20851441405707477, 'financial spending': 0.20851441405707477, 'details financial': 0.20851441405707477}\n",
      "Text 3: 'This is a very short sentence.'\n",
      "  Keywords (TF-IDF): {'short sentence': 0.5773502691896258, 'sentence': 0.5773502691896258, 'short': 0.5773502691896258}\n",
      "Text 4: '         '\n",
      "  Keywords (TF-IDF): {}\n",
      "Text 5: 'the a an of'\n",
      "  Keywords (TF-IDF): {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\.conda\\envs\\geospatial\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# spaCy's stop words list can be quite comprehensive.\n",
    "# Ensure spaCy is loaded if you plan to use its stop words directly here,\n",
    "# otherwise, TfidfVectorizer has its own 'english' stop word list.\n",
    "# For simplicity, if nlp (spaCy model) is loaded, we will use its stop words.\n",
    "# Otherwise, TfidfVectorizer will use its default English stop words.\n",
    "\n",
    "def get_keywords_tfidf(text_list, num_keywords=10):\n",
    "    \"\"\"\n",
    "    Extracts keywords from a list of texts (intended for a single document's content,\n",
    "    passed as a list) using TF-IDF.\n",
    "    Returns a dictionary of keywords and their TF-IDF scores.\n",
    "    \"\"\"\n",
    "    if not text_list or not all(isinstance(s, str) and s.strip() for s in text_list if s):\n",
    "        # print(\"Warning: Empty or invalid text_list provided to get_keywords_tfidf.\")\n",
    "        return {} # Return empty dict if input is problematic (e.g., list of empty strings)\n",
    "    \n",
    "    valid_texts = [s for s in text_list if s and s.strip()]\n",
    "    if not valid_texts:\n",
    "        # print(\"Warning: No valid texts in text_list for get_keywords_tfidf after stripping.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        current_stop_words = 'english' # Default for TfidfVectorizer\n",
    "        if nlp and hasattr(nlp.Defaults, 'stop_words'): # Check if spaCy's nlp object is loaded and has stop words\n",
    "            current_stop_words = list(nlp.Defaults.stop_words)\n",
    "        \n",
    "        # Initialize TfidfVectorizer\n",
    "        # token_pattern ensures we capture words with hyphens and words that are at least 2 characters long.\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words=current_stop_words,\n",
    "            max_features=1000,         # Limit the number of features (vocabulary size)\n",
    "            ngram_range=(1, 2),        # Consider unigrams and bigrams\n",
    "            token_pattern=r'(?u)\\b\\w[\\w-]*\\w\\b|\\b\\w\\w+\\b' # More robust token pattern\n",
    "        )\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(valid_texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Since this function is often called with a single document's content (as a list containing one string),\n",
    "        # we'll process the first (and likely only) document in the matrix.\n",
    "        doc_vector = tfidf_matrix[0] # Get the TF-IDF vector for the first document\n",
    "        \n",
    "        # Create a dictionary of keywords and their scores\n",
    "        tfidf_scores_dict = {}\n",
    "        # doc_vector.nonzero()[1] gives the indices of non-zero elements (features present in the doc)\n",
    "        for col_idx in doc_vector.nonzero()[1]:\n",
    "            tfidf_scores_dict[feature_names[col_idx]] = doc_vector[0, col_idx]\n",
    "            \n",
    "        # Sort keywords by TF-IDF score in descending order and take the top N\n",
    "        sorted_keywords = sorted(tfidf_scores_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        return dict(sorted_keywords[:num_keywords])\n",
    "\n",
    "    except ValueError as e:\n",
    "        # This can happen if the vocabulary is empty after stop word removal (e.g., very short text with only stop words)\n",
    "        # print(f\"TF-IDF ValueError (e.g., empty vocabulary after stop words): {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred in get_keywords_tfidf: {e}\")\n",
    "        return {}\n",
    "\n",
    "print(\"get_keywords_tfidf function defined.\")\n",
    "\n",
    "# --- Test cases for get_keywords_tfidf ---\n",
    "print(\"\\n--- Testing get_keywords_tfidf function ---\")\n",
    "test_keyword_texts = [\n",
    "    [\"Renewable energy sources like solar power and wind turbines are crucial for combating climate change. Investment in green technology is increasing.\"],\n",
    "    [\"The report details financial spending and budget allocations for the upcoming fiscal year. It's a very dry document about economics.\"],\n",
    "    [\"This is a very short sentence.\"],\n",
    "    [\"         \"], # Text with only whitespace\n",
    "    [\"the a an of\"] # Text with only common stop words (using default TfidfVectorizer list)\n",
    "]\n",
    "\n",
    "if nlp: # Ensure nlp object is available for stop words\n",
    "    for i, text_as_list in enumerate(test_keyword_texts):\n",
    "        keywords_result = get_keywords_tfidf(text_as_list)\n",
    "        # Joining list for printing, as the function expects a list of strings\n",
    "        print(f\"Text {i+1}: '{' '.join(text_as_list if text_as_list else [''])}'\")\n",
    "        print(f\"  Keywords (TF-IDF): {keywords_result}\")\n",
    "else:\n",
    "    print(\"spaCy nlp model not loaded. TF-IDF will use default stop words. Results might differ slightly or fail if spaCy stop words were strictly intended.\")\n",
    "    # Re-running tests with 'english' stop words explicitly for this case\n",
    "    for i, text_as_list in enumerate(test_keyword_texts):\n",
    "        # Temporarily force TfidfVectorizer to use 'english' if nlp isn't loaded, for test consistency\n",
    "        original_nlp_status = nlp # store original status\n",
    "        nlp = None # temporarily disable nlp to force 'english' stop words in function\n",
    "        keywords_result = get_keywords_tfidf(text_as_list)\n",
    "        nlp = original_nlp_status # restore\n",
    "        print(f\"Text {i+1}: '{' '.join(text_as_list if text_as_list else [''])}'\")\n",
    "        print(f\"  Keywords (TF-IDF) (using default 'english' stop words): {keywords_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97eba923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_text_content function defined.\n",
      "\n",
      "--- Testing process_text_content function ---\n",
      "\n",
      "Processing Comprehensive Text: 'TechGlobal Corp. announced a groundbreaking solar panel with 50% increased efficiency. This innovation, developed in their Berlin labs by Dr. Eva Rostova, is expected to significantly impact the renewable energy market in Europe. Analysts are very optimistic about this fantastic news, despite some initial high costs.'\n",
      "  Sentiment: {'score': 0.9259, 'label': 'positive'}\n",
      "  Keywords: {'high costs': 0.12803687993289598, 'initial high': 0.12803687993289598, 'despite initial': 0.12803687993289598, 'news despite': 0.12803687993289598, 'fantastic news': 0.12803687993289598, 'optimistic fantastic': 0.12803687993289598, 'analysts optimistic': 0.12803687993289598, 'europe analysts': 0.12803687993289598, 'market europe': 0.12803687993289598, 'energy market': 0.12803687993289598}\n",
      "  Entities: {'TechGlobal Corp.': 'ORG', '50%': 'PERCENT', 'Berlin': 'GPE', 'Eva Rostova': 'PERSON', 'Europe': 'LOC'}\n",
      "\n",
      "Processing Empty Text: '   '\n",
      "  Sentiment (empty): {'score': 0.0, 'label': 'neutral'}\n",
      "  Keywords (empty): {}\n",
      "  Entities (empty): {}\n",
      "\n",
      "Processing None input:\n",
      "  Sentiment (None): {'score': 0.0, 'label': 'neutral'}\n",
      "  Keywords (None): {}\n",
      "  Entities (None): {}\n"
     ]
    }
   ],
   "source": [
    "def process_text_content(text_content):\n",
    "    \"\"\"\n",
    "    Processes a single piece of text content to extract sentiment, keywords, and entities.\n",
    "    Returns a tuple: (sentiment_dict, keywords_dict, entities_dict)\n",
    "    \"\"\"\n",
    "    if not text_content or not isinstance(text_content, str) or not text_content.strip():\n",
    "        # Return default structures for empty/invalid input to avoid errors downstream\n",
    "        empty_sentiment = {'score': 0.0, 'label': 'neutral'}\n",
    "        empty_keywords = {}\n",
    "        empty_entities = {}\n",
    "        # print(\"Warning: Empty or invalid text_content provided to process_text_content.\")\n",
    "        return empty_sentiment, empty_keywords, empty_entities\n",
    "        \n",
    "    # Ensure NLP tools are loaded\n",
    "    if not nlp or not vader_analyzer:\n",
    "        print(\"Error: NLP models (spaCy/VADER) not loaded in process_text_content.\")\n",
    "        # Return default structures if tools aren't ready\n",
    "        return {'score': 0.0, 'label': 'neutral'}, {}, {}\n",
    "\n",
    "    # 1. Get Sentiment\n",
    "    sentiment = get_sentiment(text_content)\n",
    "    \n",
    "    # 2. Get Entities\n",
    "    entities = get_entities(text_content)\n",
    "    \n",
    "    # 3. Get Keywords\n",
    "    # Remember, get_keywords_tfidf expects a list of strings.\n",
    "    # For a single article's content, we pass it as a list containing that one string.\n",
    "    keywords = get_keywords_tfidf([text_content]) \n",
    "    \n",
    "    return sentiment, keywords, entities\n",
    "\n",
    "print(\"process_text_content function defined.\")\n",
    "\n",
    "# --- Test cases for process_text_content ---\n",
    "print(\"\\n--- Testing process_text_content function ---\")\n",
    "\n",
    "# A more comprehensive test sentence combining different elements\n",
    "comprehensive_text = (\"TechGlobal Corp. announced a groundbreaking solar panel with 50% increased efficiency. \"\n",
    "                      \"This innovation, developed in their Berlin labs by Dr. Eva Rostova, \"\n",
    "                      \"is expected to significantly impact the renewable energy market in Europe. \"\n",
    "                      \"Analysts are very optimistic about this fantastic news, despite some initial high costs.\")\n",
    "\n",
    "empty_text_test = \"   \" # Test with only whitespace\n",
    "\n",
    "if nlp and vader_analyzer: # Ensure tools are loaded before testing\n",
    "    print(f\"\\nProcessing Comprehensive Text: '{comprehensive_text}'\")\n",
    "    s_comp, k_comp, e_comp = process_text_content(comprehensive_text)\n",
    "    print(f\"  Sentiment: {s_comp}\")\n",
    "    print(f\"  Keywords: {k_comp}\")\n",
    "    print(f\"  Entities: {e_comp}\")\n",
    "\n",
    "    print(f\"\\nProcessing Empty Text: '{empty_text_test}'\")\n",
    "    s_empty, k_empty, e_empty = process_text_content(empty_text_test)\n",
    "    print(f\"  Sentiment (empty): {s_empty}\")\n",
    "    print(f\"  Keywords (empty): {k_empty}\")\n",
    "    print(f\"  Entities (empty): {e_empty}\")\n",
    "\n",
    "    print(f\"\\nProcessing None input:\")\n",
    "    s_none, k_none, e_none = process_text_content(None) # Test None\n",
    "    print(f\"  Sentiment (None): {s_none}\")\n",
    "    print(f\"  Keywords (None): {k_none}\")\n",
    "    print(f\"  Entities (None): {e_none}\")\n",
    "\n",
    "else:\n",
    "    print(\"spaCy nlp model or VADER analyzer not loaded. Skipping process_text_content tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea284122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing a Sample of Actual Scraped Articles (from CSV) ---\n",
      "\n",
      "Processing Article URL: https://www.theguardian.com/environment/2025/may/22/revealed-uranium-from-uk-nuclear-fuel-factory-dumped-into-protected-ribble-estuary\n",
      "Title: Revealed: three tonnes of uranium legally dumped in protected English estuary in...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\.conda\\envs\\geospatial\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment: {'score': 0.9928, 'label': 'positive'}\n",
      "  Keywords: {'environment': 0.2885342830935783, 'uranium': 0.25246749770688104, 'fuels': 0.18033392693348646, 'springfields': 0.18033392693348646, 'discharges': 0.18033392693348646, 'environment agency': 0.16230053424013782, 'said': 0.16230053424013782, 'agency': 0.16230053424013782, 'springfields fuels': 0.14426714154678916, 'radioactivity': 0.14426714154678916}\n",
      "  Entities: {'The Environment Agency': 'ORG', 'three tonnes': 'QUANTITY', 'England': 'GPE', 'the past nine years': 'DATE', 'Guardian': 'ORG', 'the Ends Report': 'ORG', 'Preston': 'ORG', 'between 2015 and 2024': 'DATE', '2015': 'DATE', '703kg': 'QUANTITY', 'Lea Town': 'PERSON', 'roughly five miles': 'QUANTITY', 'several million': 'CARDINAL', '11': 'CARDINAL', 'about 800': 'CARDINAL', 'SPA': 'ORG', 'Ramsar': 'PERSON', 'November 2024': 'DATE', '2023': 'DATE', 'Springfields Fuels': 'PERSON', 'approximately 4%': 'PERCENT', 'Dr Ian Fairlile': 'ORG', 'UK': 'GPE', '2009': 'DATE', 'the Environment Agency': 'ORG', 'the Ribble and Alt estuaries SPA': 'WORK_OF_ART', '40': 'CARDINAL', 'more than 10': 'CARDINAL', 'annual': 'DATE', '0.04': 'CARDINAL', '0.1': 'CARDINAL', '1tn': 'ORDINAL', 'One': 'CARDINAL', 'one': 'CARDINAL', 'second': 'ORDINAL', 'six years ago': 'DATE', 'the Environment Agency’s': 'ORG', 'Patrick Byrne': 'PERSON', 'Liverpool John Moores University': 'ORG', 'Down': 'PERSON', 'week': 'DATE', 'Enter': 'PERSON', 'Newsletters': 'PRODUCT', 'Google': 'ORG', 'the Google Privacy Policy and Terms of Service': 'ORG', 'Doug Parr': 'PERSON', 'Greenpeace UK': 'ORG', 'Springfields Fuels Limited': 'ORG', 'Springfield Fuels': 'PERSON', 'first': 'ORDINAL', 'the British Geological Survey': 'ORG', 'BGS': 'ORG', '2002': 'DATE', 'Springfields': 'PERSON', '60μg': 'ORDINAL', '3-4μg': 'DATE', 'Russian': 'NORP', '24GW': 'CARDINAL', '2050': 'DATE'}\n",
      "\n",
      "Processing Article URL: https://www.theguardian.com/environment/2025/may/22/marine-heatwave-sea-temperatures-devon-cornwall-ireland\n",
      "Title: ‘Unprecedented’ marine heatwave hits waters around Devon, Cornwall and Ireland...\n",
      "  Sentiment: {'score': 0.9942, 'label': 'positive'}\n",
      "  Keywords: {'marine': 0.36607149342253503, 'temperatures': 0.292857194738028, 'said': 0.21964289605352103, 'sea': 0.19523812982535202, 'uk': 0.170833363597183, 'heatwave': 0.146428597369014, 'marine heatwave': 0.12202383114084502, 'time': 0.12202383114084502, 'sea temperatures': 0.09761906491267601, 'marine heatwaves': 0.09761906491267601}\n",
      "  Entities: {'UK': 'GPE', 'Ireland': 'GPE', 'as much as 4C': 'CARDINAL', 'the spring': 'DATE', 'Marine': 'PERSON', 'Devon': 'GPE', 'Cornwall': 'GPE', 'the west coast': 'LOC', 'early in the year': 'DATE', 'Dr Manuela Truebano': 'PERSON', 'the University of Plymouth': 'ORG', 'this time of year': 'DATE', '2C': 'ORG', '4C': 'CARDINAL', 'Dan Smale': 'PERSON', 'the Marine Biological Association': 'ORG', '11C-12C': 'DATE', 'June 2023': 'DATE', 'Truebano': 'PERSON', 'March': 'DATE', 'almost two months': 'DATE', 'the summer months': 'DATE', 'spring': 'DATE', 'later in the year': 'DATE', 'Smale': 'GPE', 'summer': 'DATE', 'Down': 'PERSON', 'week': 'DATE', 'Enter': 'PERSON', 'Newsletters': 'PRODUCT', 'Google': 'ORG', 'the Google Privacy Policy and Terms of Service': 'ORG', 'The Met Office': 'ORG', 'more than two months': 'DATE', 'early March': 'DATE', 'May': 'DATE', 'mid February': 'DATE', 'one month earlier': 'DATE', 'winter': 'DATE', '2019': 'DATE', 'more than 50%': 'PERCENT', 'the 30 years to 2016': 'DATE', '1925-54': 'DATE', 'Jonathan Tinker': 'PERSON', 'the Met Office': 'ORG', '2.5C': 'CARDINAL', '2050': 'DATE'}\n",
      "\n",
      "Processing Article URL: https://www.theguardian.com/environment/2025/may/22/kenya-mathenge-desertification-invasive-plants-neltuma-prosopis-juliflora-samburu-pastoralists-aoe\n",
      "Title: How an idealistic tree-planting project turned into Kenya’s toxic, thorny nightm...\n",
      "  Sentiment: {'score': 0.7615, 'label': 'positive'}\n",
      "  Keywords: {'mathenge': 0.38194099085590216, 'plant': 0.19097049542795108, 'pods': 0.17187344588515596, 'says': 0.17187344588515596, 'livestock': 0.17187344588515596, 'spread': 0.13367934679956575, 'kenya': 0.13367934679956575, 'lmakato': 0.11458229725677065, 'feed': 0.09548524771397554, 'animals': 0.09548524771397554}\n",
      "  Entities: {'John Lmakato': 'PERSON', 'Lerata': 'GPE', 'Mount Ololokwe': 'LOC', 'Kenya': 'GPE', 'Samburu': 'GPE', 'Lmakato': 'PERSON', 'three years ago': 'DATE', '193': 'CARDINAL', 'Laikipia': 'GPE', '200': 'CARDINAL', 'only seven': 'CARDINAL', 'One': 'CARDINAL', '48': 'DATE', '1948': 'DATE', 'South America': 'LOC', 'east Africa': 'GPE', 'the 1970s': 'DATE', 'drylands': 'GPE', 'the UN Food and Agricultural Organization': 'ORG', 'The Kenya Forestry Research Institute': 'ORG', 'Kefri': 'GPE', '2 million hectares': 'QUANTITY', '7,700 sq miles': 'QUANTITY', 'up to 15%': 'PERCENT', 'Ramadhan Golicha': 'PERSON', '2006': 'DATE', 'Kenyan': 'NORP', 'Baringo': 'GPE', 'one': 'CARDINAL', 'Davis Ikiror': 'PERSON', 'Somalia': 'GPE', 'VSF': 'ORG', 'more than two decades': 'DATE', 'more than 60%': 'PERCENT', '30%': 'PERCENT', '2008': 'DATE', 'Isiolo': 'ORG', 'acacia pods': 'PERSON', '25': 'CARDINAL', 'VSF Suisse': 'ORG', 'the University of Nairobi': 'ORG', 'Samburu and Isiolo': 'ORG', 'Global Dispatch Free': 'ORG', 'Get': 'PERSON', 'Newsletters': 'PRODUCT', 'Google': 'ORG', 'the Google Privacy Policy and Terms of Service': 'ORG', 'Douglas Machuchu': 'PERSON', 'first': 'ORDINAL', 'the entire dry season': 'DATE', 'Martina Lenanyangerra': 'PERSON', 'nearly 180 miles': 'QUANTITY', '300km': 'QUANTITY', 'June last year': 'DATE', 'Edward Musya': 'PERSON', 'Merti': 'GPE', 'more than 20': 'CARDINAL', 'Nicolas Echuman': 'PERSON', '88': 'DATE', 'Ngaremara': 'GPE', 'the early 1980s': 'DATE', 'Musya': 'PERSON', 'more than 95%': 'PERCENT', 'as far as': 'CARDINAL', '35 metres': 'QUANTITY', 'a’Boru': 'PERSON', 'Rift Valley': 'LOC', 'kala': 'GPE', 'Phoebe Weston': 'PERSON', 'Patrick Greenfield': 'PERSON', 'Guardian': 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "# This cell is optional.\n",
    "# If you saved your scraped data from Notebook 2, you can load it:\n",
    "try:\n",
    "    scraped_df_for_nlp_test = pd.read_csv(\"scraped_articles_combined_notebook.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"CSV file 'scraped_articles_combined_notebook.csv' not found. Skipping this optional test.\")\n",
    "    scraped_df_for_nlp_test = pd.DataFrame() # Create empty DataFrame to avoid errors\n",
    "\n",
    "if not scraped_df_for_nlp_test.empty and nlp and vader_analyzer:\n",
    "    print(\"\\n--- Processing a Sample of Actual Scraped Articles (from CSV) ---\")\n",
    "    # Take a small sample, e.g., the first 2-3 articles that have content\n",
    "    sample_for_processing = scraped_df_for_nlp_test[scraped_df_for_nlp_test['full_content'].notna() & (scraped_df_for_nlp_test['full_content'].str.strip() != '')].head(3)\n",
    "    \n",
    "    if not sample_for_processing.empty:\n",
    "        for index, row in sample_for_processing.iterrows():\n",
    "            print(f\"\\nProcessing Article URL: {row['url']}\")\n",
    "            print(f\"Title: {row['title'][:80]}...\")\n",
    "            \n",
    "            content_to_process = row['full_content']\n",
    "            sentiment_result, keywords_result, entities_result = process_text_content(content_to_process)\n",
    "            \n",
    "            print(f\"  Sentiment: {sentiment_result}\")\n",
    "            print(f\"  Keywords: {keywords_result}\")\n",
    "            print(f\"  Entities: {entities_result}\")\n",
    "    else:\n",
    "        print(\"No suitable articles with content found in the loaded CSV for processing.\")\n",
    "elif not nlp or not vader_analyzer:\n",
    "    print(\"NLP tools (spaCy/VADER) not loaded. Skipping processing of scraped articles.\")\n",
    "else:\n",
    "    print(\"Skipping optional processing of scraped articles from CSV as no data was loaded.\")\n",
    "\n",
    "# --- Fallback: Process a predefined test article if no CSV is loaded ---\n",
    "# if not ('scraped_df_for_nlp_test' in locals() and not scraped_df_for_nlp_test.empty) and nlp and vader_analyzer:\n",
    "#     print(\"\\n--- Processing a Predefined Test Article (as CSV was not loaded/empty) ---\")\n",
    "#     test_article_content_real = \"\"\"\n",
    "#     Major advancements in geothermal energy extraction were reported today from a research facility in Iceland.\n",
    "#     The new technique, pioneered by Dr. Aris Thorne of GeoDynamics Inc., promises to double efficiency.\n",
    "#     This could make geothermal power a more viable option for countries like Japan and the United States.\n",
    "#     The financial markets reacted positively, with GeoDynamics Inc. (GEO) shares surging by 15% on the New York Stock Exchange.\n",
    "#     Environmental groups have lauded this as a significant step towards sustainable energy independence.\n",
    "#     The project received initial funding of $50 million from the Global Environment Fund.\n",
    "#     \"\"\"\n",
    "#     sentiment_result, keywords_result, entities_result = process_text_content(test_article_content_real)\n",
    "#     print(f\"Test Article Content: '{test_article_content_real[:100]}...'\")\n",
    "#     print(f\"  Sentiment: {sentiment_result}\")\n",
    "#     print(f\"  Keywords: {keywords_result}\")\n",
    "#     print(f\"  Entities: {entities_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb9d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
