{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35398fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports for web scraping are complete.\n",
      "Scraping configurations (RSS feeds, Reddit targets, post limits) are loaded.\n",
      "Target RSS Feeds for this session: ['Guardian Environment', 'Ars Technica']\n",
      "Target Subreddits for this session: ['RenewableEnergy'] (Limit: 5 posts each)\n"
     ]
    }
   ],
   "source": [
    "import requests                         # For making HTTP requests to fetch web content\n",
    "from bs4 import BeautifulSoup          # For parsing HTML (though less used here as feedparser/newspaper3k handle much of it)\n",
    "import feedparser                       # Specifically for parsing RSS/Atom feeds\n",
    "from newspaper import Article as NewspaperArticle, ArticleException # For extracting main content from news article URLs\n",
    "from datetime import datetime, timezone  # For handling and standardizing dates and times\n",
    "import time                             # For adding polite delays between requests\n",
    "import pandas as pd                     # For displaying data structures like DataFrames in the notebook\n",
    "\n",
    "# --- Scraping Configuration ---\n",
    "# Define the RSS feeds to be scraped.\n",
    "# Ensure these URLs are active and relevant to the project's analysis topic.\n",
    "# Note: It's good practice to verify feed validity regularly as sources can change.\n",
    "NEWS_RSS_FEEDS = {\n",
    "    \"Guardian Environment\": \"https://www.theguardian.com/environment/rss\", # Known good, relevant feed\n",
    "    \"Ars Technica\": \"http://feeds.arstechnica.com/arstechnica/index/\"     # General tech news, good for testing scraper\n",
    "    # The Reuters Environment feed previously considered was found to be inactive.\n",
    "    # Add other verified RSS feeds relevant to \"Advancements in Renewable Energy Technologies\".\n",
    "}\n",
    "\n",
    "# Define the Reddit subreddits to be scraped.\n",
    "REDDIT_SUBREDDITS = {\n",
    "    \"RenewableEnergySub\": \"RenewableEnergy\"  # Internal key mapping to the actual subreddit name\n",
    "}\n",
    "# Limit the number of recent posts fetched from each Reddit source per cycle.\n",
    "# Kept small for efficient testing within the notebook; can be increased for more data.\n",
    "REDDIT_POST_LIMIT = 5\n",
    "\n",
    "print(\"Imports for web scraping are complete.\")\n",
    "print(\"Scraping configurations (RSS feeds, Reddit targets, post limits) are loaded.\")\n",
    "print(f\"Target RSS Feeds for this session: {list(NEWS_RSS_FEEDS.keys())}\")\n",
    "print(f\"Target Subreddits for this session: {list(REDDIT_SUBREDDITS.values())} (Limit: {REDDIT_POST_LIMIT} posts each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f5f5413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date parsing utility function 'parse_datetime' defined.\n",
      "\n",
      "--- Testing 'parse_datetime' with various date formats: ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Original Value</th>\n",
       "      <th>Parsed Datetime</th>\n",
       "      <th>Output Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standard RSS with offset</td>\n",
       "      <td>Tue, 21 May 2025 10:00:00 +0000</td>\n",
       "      <td>2025-05-21 10:00:00+00:00</td>\n",
       "      <td>&lt;class 'datetime.datetime'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standard RSS with timezone name</td>\n",
       "      <td>Tue, 21 May 2025 10:00:00 GMT</td>\n",
       "      <td>2025-05-21 10:00:00+00:00</td>\n",
       "      <td>&lt;class 'datetime.datetime'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISO 8601 with offset</td>\n",
       "      <td>2025-05-21T10:00:00+00:00</td>\n",
       "      <td>2025-05-21 10:00:00+00:00</td>\n",
       "      <td>&lt;class 'datetime.datetime'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unix timestamp (represents 2023-03-21 12:00:00...</td>\n",
       "      <td>1679400000</td>\n",
       "      <td>2023-03-21 12:00:00+00:00</td>\n",
       "      <td>&lt;class 'datetime.datetime'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None input</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;class 'NoneType'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unparseable string</td>\n",
       "      <td>Invalid Date String</td>\n",
       "      <td>2025-05-23 05:19:30.092392+00:00</td>\n",
       "      <td>&lt;class 'datetime.datetime'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feedparser.struct_time object (simulated)</td>\n",
       "      <td>time.struct_time(tm_year=2023, tm_mon=3, tm_md...</td>\n",
       "      <td>2023-03-21 06:30:00+00:00</td>\n",
       "      <td>&lt;class 'datetime.datetime'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0                           Standard RSS with offset   \n",
       "1                    Standard RSS with timezone name   \n",
       "2                               ISO 8601 with offset   \n",
       "3  Unix timestamp (represents 2023-03-21 12:00:00...   \n",
       "4                                         None input   \n",
       "5                                 Unparseable string   \n",
       "6          feedparser.struct_time object (simulated)   \n",
       "\n",
       "                                      Original Value  \\\n",
       "0                    Tue, 21 May 2025 10:00:00 +0000   \n",
       "1                      Tue, 21 May 2025 10:00:00 GMT   \n",
       "2                          2025-05-21T10:00:00+00:00   \n",
       "3                                         1679400000   \n",
       "4                                               None   \n",
       "5                                Invalid Date String   \n",
       "6  time.struct_time(tm_year=2023, tm_mon=3, tm_md...   \n",
       "\n",
       "                   Parsed Datetime                  Output Type  \n",
       "0        2025-05-21 10:00:00+00:00  <class 'datetime.datetime'>  \n",
       "1        2025-05-21 10:00:00+00:00  <class 'datetime.datetime'>  \n",
       "2        2025-05-21 10:00:00+00:00  <class 'datetime.datetime'>  \n",
       "3        2023-03-21 12:00:00+00:00  <class 'datetime.datetime'>  \n",
       "4                              NaT           <class 'NoneType'>  \n",
       "5 2025-05-23 05:19:30.092392+00:00  <class 'datetime.datetime'>  \n",
       "6        2023-03-21 06:30:00+00:00  <class 'datetime.datetime'>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, timezone # Ensure these are imported if not already in a previous cell of this notebook\n",
    "import time                         # Required for time.mktime\n",
    "\n",
    "def parse_datetime(date_string):\n",
    "    \"\"\"\n",
    "    Parses various common date string formats and returns a timezone-aware \n",
    "    datetime object, standardized to UTC.\n",
    "    \n",
    "    Handles formats commonly found in RSS feeds (RFC 822, RFC 3339 variations), \n",
    "    ISO 8601 timestamps, and Unix timestamps (e.g., from Reddit).\n",
    "    \"\"\"\n",
    "    if not date_string:\n",
    "        return None # Return None for empty or None input\n",
    "\n",
    "    # Prioritized list of common date formats to attempt parsing\n",
    "    common_formats = [\n",
    "        '%a, %d %b %Y %H:%M:%S %z',        # e.g., 'Tue, 21 May 2025 10:00:00 +0000'\n",
    "        '%a, %d %b %Y %H:%M:%S %Z',        # e.g., 'Tue, 21 May 2025 10:00:00 GMT'\n",
    "        '%Y-%m-%dT%H:%M:%S%z',            # e.g., '2025-05-21T10:00:00+00:00' (ISO 8601)\n",
    "        '%Y-%m-%dT%H:%M:%S.%f%z',        # e.g., '2025-05-21T10:00:00.123456+00:00' (ISO 8601 with microseconds)\n",
    "        '%Y-%m-%d %H:%M:%S',              # e.g., '2025-05-21 10:00:00' (Assumed UTC if no timezone)\n",
    "        '%d %b %Y %H:%M:%S %Z',            # Less common RSS variant\n",
    "    ]\n",
    "    \n",
    "    dt_object = None\n",
    "\n",
    "    # Attempt to parse if input is a Unix timestamp (integer or float)\n",
    "    if isinstance(date_string, (int, float)):\n",
    "        try:\n",
    "            return datetime.fromtimestamp(date_string, timezone.utc)\n",
    "        except (ValueError, TypeError):\n",
    "            pass # If conversion fails, proceed to string parsing methods\n",
    "\n",
    "    # Attempt to parse if input is a string\n",
    "    if isinstance(date_string, str):\n",
    "        for fmt in common_formats:\n",
    "            try:\n",
    "                dt_object = datetime.strptime(date_string, fmt)\n",
    "                # Standardize to UTC: If datetime object is naive (no timezone info), assume UTC.\n",
    "                if dt_object.tzinfo is None or dt_object.tzinfo.utcoffset(dt_object) is None:\n",
    "                    dt_object = dt_object.replace(tzinfo=timezone.utc)\n",
    "                return dt_object\n",
    "            except ValueError:\n",
    "                continue # If current format fails, try the next one\n",
    "    \n",
    "    # Attempt to parse if input is a feedparser.struct_time object\n",
    "    elif hasattr(date_string, 'tm_year'): \n",
    "         try:\n",
    "            # Convert struct_time to Unix timestamp, then to datetime object\n",
    "            return datetime.fromtimestamp(time.mktime(date_string), timezone.utc)\n",
    "         except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "    # Fallback: If all parsing attempts fail, log a warning (optional) and return a default.\n",
    "    # For critical applications, raising an error or returning None might be preferred.\n",
    "    # print(f\"Warning: Date string '{date_string}' could not be parsed into a known format. Defaulting to current UTC time.\")\n",
    "    return datetime.now(timezone.utc) \n",
    "\n",
    "print(\"Date parsing utility function 'parse_datetime' defined.\")\n",
    "\n",
    "# --- Test Cases for parse_datetime ---\n",
    "print(\"\\n--- Testing 'parse_datetime' with various date formats: ---\")\n",
    "test_dates_and_types = [\n",
    "    (\"Tue, 21 May 2025 10:00:00 +0000\", \"Standard RSS with offset\"),\n",
    "    (\"Tue, 21 May 2025 10:00:00 GMT\",   \"Standard RSS with timezone name\"),\n",
    "    (\"2025-05-21T10:00:00+00:00\",       \"ISO 8601 with offset\"),\n",
    "    (1679400000,                        \"Unix timestamp (represents 2023-03-21 12:00:00 UTC)\"),\n",
    "    (None,                              \"None input\"),\n",
    "    (\"Invalid Date String\",             \"Unparseable string\"),\n",
    "    (time.gmtime(1679400000),           \"feedparser.struct_time object (simulated)\") # Test for struct_time\n",
    "]\n",
    "\n",
    "results = []\n",
    "for original_value, description in test_dates_and_types:\n",
    "    parsed_datetime = parse_datetime(original_value)\n",
    "    results.append({\n",
    "        \"Description\": description,\n",
    "        \"Original Value\": str(original_value), # Convert all to string for consistent display\n",
    "        \"Parsed Datetime\": parsed_datetime,\n",
    "        \"Output Type\": str(type(parsed_datetime))\n",
    "    })\n",
    "\n",
    "# Display results in a more structured way if pandas is available and preferred\n",
    "try:\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df) # display() is a Jupyter-specific function for rich display\n",
    "except ImportError:\n",
    "    for res in results:\n",
    "        print(f\"Description: {res['Description']}\")\n",
    "        print(f\"  Original: {res['Original Value']}\")\n",
    "        print(f\"  Parsed:   {res['Parsed Datetime']} (Type: {res['Output Type']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74743c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content extraction function 'fetch_article_content' defined.\n",
      "\n",
      "--- Testing 'fetch_article_content' with various URLs: ---\n",
      "\n",
      "Attempting to fetch: Valid Article (Example - The Guardian, if still accessible) - URL: https://www.theguardian.com/environment/2023/oct/26/hope-for-coral-great-barrier-reef-shows-tentative-signs-of-recovery\n",
      "  Could not retrieve valid content.\n",
      "\n",
      "Attempting to fetch: Section Page (Reuters Technology - newspaper3k might struggle) - URL: https://www.reuters.com/technology/\n",
      "  Could not retrieve valid content.\n",
      "\n",
      "Attempting to fetch: Non-Existent Page (example.com) - URL: https://www.example.com/this-page-does-not-exist-123\n",
      "  Could not retrieve valid content.\n",
      "\n",
      "Attempting to fetch: Invalid URL (None) - URL: None\n",
      "  Could not retrieve valid content.\n",
      "\n",
      "Attempting to fetch: Invalid URL (Empty String) - URL: \n",
      "  Could not retrieve valid content.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Content Extracted</th>\n",
       "      <th>Content Preview (first 70 chars)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Valid Article (Example - The Guardian, if stil...</td>\n",
       "      <td>https://www.theguardian.com/environment/2023/o...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Section Page (Reuters Technology - newspaper3k...</td>\n",
       "      <td>https://www.reuters.com/technology/</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-Existent Page (example.com)</td>\n",
       "      <td>https://www.example.com/this-page-does-not-exi...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Invalid URL (None)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Invalid URL (Empty String)</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  Valid Article (Example - The Guardian, if stil...   \n",
       "1  Section Page (Reuters Technology - newspaper3k...   \n",
       "2                    Non-Existent Page (example.com)   \n",
       "3                                 Invalid URL (None)   \n",
       "4                         Invalid URL (Empty String)   \n",
       "\n",
       "                                                 URL Publication Date  \\\n",
       "0  https://www.theguardian.com/environment/2023/o...             None   \n",
       "1                https://www.reuters.com/technology/             None   \n",
       "2  https://www.example.com/this-page-does-not-exi...             None   \n",
       "3                                               None             None   \n",
       "4                                                                None   \n",
       "\n",
       "   Content Extracted Content Preview (first 70 chars)  \n",
       "0              False                              N/A  \n",
       "1              False                              N/A  \n",
       "2              False                              N/A  \n",
       "3              False                              N/A  \n",
       "4              False                              N/A  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from newspaper import Article as NewspaperArticle, ArticleException # Ensure this import is at the top of the cell or notebook\n",
    "from datetime import datetime, timezone # Ensure datetime and timezone are available\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    \"\"\"\n",
    "    Fetches, parses, and extracts the main textual content and publication date \n",
    "    from a news article at the given URL using the newspaper3k library.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the news article.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (text, pub_date)\n",
    "                 - text (str): The extracted main text of the article.\n",
    "                 - pub_date (datetime.datetime): The extracted publication date, standardized to UTC.\n",
    "                 Returns (None, None) if fetching or parsing fails, or if the URL is invalid.\n",
    "    \"\"\"\n",
    "    if not url or not isinstance(url, str) or not url.startswith(('http://', 'https://')):\n",
    "        # print(f\"Debug: Invalid or empty URL provided: {url}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Initialize the Article object from newspaper3k.\n",
    "        # fetch_images=False: Disables downloading of images, speeding up the process.\n",
    "        # memoize_articles=False: Disables caching of articles to disk. Useful if content might change\n",
    "        #                         or to avoid using disk cache during development across sessions.\n",
    "        article_obj = NewspaperArticle(url, fetch_images=False, memoize_articles=False)\n",
    "        \n",
    "        # Download the HTML content of the article.\n",
    "        article_obj.download()\n",
    "        \n",
    "        # Parse the downloaded HTML to extract data.\n",
    "        article_obj.parse()\n",
    "        \n",
    "        text = article_obj.text\n",
    "        \n",
    "        # Extract and standardize the publication date.\n",
    "        pub_date = article_obj.publish_date\n",
    "        if pub_date:\n",
    "            # If the extracted date is naive (no timezone info), assume it's UTC.\n",
    "            if pub_date.tzinfo is None or pub_date.tzinfo.utcoffset(pub_date) is None:\n",
    "                pub_date = pub_date.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        return text, pub_date\n",
    "        \n",
    "    except ArticleException as e:\n",
    "        # newspaper3k specific exception for issues during download or parsing.\n",
    "        # print(f\"Debug: Newspaper3k ArticleException for URL '{url}': {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors during the process.\n",
    "        # print(f\"Debug: Unexpected error fetching/parsing article URL '{url}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"Article content extraction function 'fetch_article_content' defined.\")\n",
    "\n",
    "# --- Test Cases for fetch_article_content ---\n",
    "print(\"\\n--- Testing 'fetch_article_content' with various URLs: ---\")\n",
    "# Note: The success of these tests depends on the current accessibility and structure \n",
    "# of these specific URLs, which can change over time.\n",
    "# Using a known, stable news article URL is best for repeatable tests if available.\n",
    "\n",
    "test_article_urls_for_content = [\n",
    "    (\"Valid Article (Example - The Guardian, if still accessible)\", \"https://www.theguardian.com/environment/2023/oct/26/hope-for-coral-great-barrier-reef-shows-tentative-signs-of-recovery\"),\n",
    "    (\"Section Page (Reuters Technology - newspaper3k might struggle)\", \"https://www.reuters.com/technology/\"),\n",
    "    (\"Non-Existent Page (example.com)\", \"https://www.example.com/this-page-does-not-exist-123\"),\n",
    "    (\"Invalid URL (None)\", None),\n",
    "    (\"Invalid URL (Empty String)\", \"\")\n",
    "]\n",
    "\n",
    "results_fetch_content = []\n",
    "for description, test_url in test_article_urls_for_content:\n",
    "    print(f\"\\nAttempting to fetch: {description} - URL: {test_url}\")\n",
    "    content, pub_date = fetch_article_content(test_url)\n",
    "    \n",
    "    result_entry = {\n",
    "        \"Description\": description,\n",
    "        \"URL\": str(test_url),\n",
    "        \"Publication Date\": pub_date,\n",
    "        \"Content Extracted\": bool(content and content.strip()),\n",
    "        \"Content Preview (first 70 chars)\": content[:70].replace('\\n', ' ') + \"...\" if content and content.strip() else \"N/A\"\n",
    "    }\n",
    "    results_fetch_content.append(result_entry)\n",
    "    \n",
    "    if content and content.strip():\n",
    "        print(f\"  Successfully extracted content.\")\n",
    "        print(f\"  Publication Date: {pub_date}\")\n",
    "    else:\n",
    "        print(f\"  Could not retrieve valid content.\")\n",
    "\n",
    "# Display results in a structured way using Pandas DataFrame\n",
    "try:\n",
    "    import pandas as pd\n",
    "    results_df_fetch = pd.DataFrame(results_fetch_content)\n",
    "    display(results_df_fetch)\n",
    "except ImportError:\n",
    "    print(\"\\n(Pandas not available, printing raw results)\")\n",
    "    for res in results_fetch_content:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19c0044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'scrape_rss_feed' function has been defined.\n",
      "\n",
      "--- Testing 'scrape_rss_feed' with configured RSS feeds: ---\n",
      "\n",
      "Attempting to process feed: Guardian Environment (https://www.theguardian.com/environment/rss)\n",
      "  Successfully processed 5 entries from 'Guardian Environment'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revealed: three tonnes of uranium legally dump...</td>\n",
       "      <td>2025-05-22 08:30:33+00:00</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘Unprecedented’ marine heatwave hits waters ar...</td>\n",
       "      <td>2025-05-22 07:57:36+00:00</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Revealed: three tonnes of uranium legally dump...   \n",
       "1  ‘Unprecedented’ marine heatwave hits waters ar...   \n",
       "\n",
       "           publication_date           source_name  \n",
       "0 2025-05-22 08:30:33+00:00  Guardian Environment  \n",
       "1 2025-05-22 07:57:36+00:00  Guardian Environment  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to process feed: Ars Technica (http://feeds.arstechnica.com/arstechnica/index/)\n",
      "  Successfully processed 5 entries from 'Ars Technica'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 3.5 years, Notepad.exe has gone from “barel...</td>\n",
       "      <td>2025-05-22 17:16:32+00:00</td>\n",
       "      <td>Ars Technica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Pentagon seems to be fed up with ULA’s roc...</td>\n",
       "      <td>2025-05-22 17:02:48+00:00</td>\n",
       "      <td>Ars Technica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  In 3.5 years, Notepad.exe has gone from “barel...   \n",
       "1  The Pentagon seems to be fed up with ULA’s roc...   \n",
       "\n",
       "           publication_date   source_name  \n",
       "0 2025-05-22 17:16:32+00:00  Ars Technica  \n",
       "1 2025-05-22 17:02:48+00:00  Ars Technica  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined RSS Scraping Test Summary (Sample) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revealed: three tonnes of uranium legally dump...</td>\n",
       "      <td>2025-05-22 08:30:33+00:00</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘Unprecedented’ marine heatwave hits waters ar...</td>\n",
       "      <td>2025-05-22 07:57:36+00:00</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How an idealistic tree-planting project turned...</td>\n",
       "      <td>2025-05-21 23:30:49+00:00</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘Waste collection is green work’: how a pro-po...</td>\n",
       "      <td>2025-05-22 08:10:29+00:00</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump’s tax bill to cost 830,000 jobs and driv...</td>\n",
       "      <td>2025-05-22 05:30:09+00:00</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Revealed: three tonnes of uranium legally dump...   \n",
       "1  ‘Unprecedented’ marine heatwave hits waters ar...   \n",
       "2  How an idealistic tree-planting project turned...   \n",
       "3  ‘Waste collection is green work’: how a pro-po...   \n",
       "4  Trump’s tax bill to cost 830,000 jobs and driv...   \n",
       "\n",
       "           publication_date           source_name  \n",
       "0 2025-05-22 08:30:33+00:00  Guardian Environment  \n",
       "1 2025-05-22 07:57:36+00:00  Guardian Environment  \n",
       "2 2025-05-21 23:30:49+00:00  Guardian Environment  \n",
       "3 2025-05-22 08:10:29+00:00  Guardian Environment  \n",
       "4 2025-05-22 05:30:09+00:00  Guardian Environment  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles processed from all tested RSS feeds: 10\n"
     ]
    }
   ],
   "source": [
    "# Assumes fetch_article_content and parse_datetime are defined in previous cells\n",
    "# Assumes REDDIT_POST_LIMIT (used here for consistent demo item limit) and NEWS_RSS_FEEDS are defined in Cell 1\n",
    "\n",
    "def scrape_rss_feed(source_name, rss_url):\n",
    "    \"\"\"\n",
    "    Scrapes articles from a given RSS feed URL.\n",
    "    It first fetches the feed content using 'requests' for better control over headers,\n",
    "    then parses it with 'feedparser'. For each feed item, it attempts to fetch the\n",
    "    full article content using the 'fetch_article_content' function (which utilizes newspaper3k).\n",
    "\n",
    "    Args:\n",
    "        source_name (str): A descriptive name for the RSS feed source.\n",
    "        rss_url (str): The URL of the RSS feed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a scraped article\n",
    "              and includes its title, URL, publication date, full content, and source name.\n",
    "              Returns an empty list if scraping fails or no articles are processed.\n",
    "    \"\"\"\n",
    "    # print(f\"Attempting to scrape RSS feed: {source_name} ({rss_url})\") # Kept for verbosity if desired\n",
    "    articles = []\n",
    "    feed_data = None # Initialize to handle potential errors before assignment\n",
    "\n",
    "    # Standard headers to mimic a browser request, can improve success rate\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'application/xml, text/xml, application/rss+xml, application/atom+xml, */*' # Be explicit about accepted types\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Fetch the raw RSS feed content using requests\n",
    "        response = requests.get(rss_url, headers=headers, timeout=15) # Added a timeout\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4XX or 5XX)\n",
    "        \n",
    "        feed_content = response.content # Use .content for binary data; feedparser handles decoding\n",
    "        \n",
    "        # Step 2: Parse the fetched content using feedparser\n",
    "        feed_data = feedparser.parse(feed_content)\n",
    "        \n",
    "        # Using REDDIT_POST_LIMIT here just to limit items for consistent notebook testing speed\n",
    "        entries_to_process = feed_data.entries[:REDDIT_POST_LIMIT] \n",
    "        # print(f\"Found {len(feed_data.entries)} entries in '{source_name}' RSS feed, processing up to {len(entries_to_process)}.\")\n",
    "\n",
    "        if not feed_data.entries:\n",
    "            # print(f\"Warning: No entries found in the feed for {source_name} at {rss_url}.\")\n",
    "            if feed_data.bozo: # Check if feedparser encountered issues (e.g., malformed XML)\n",
    "                # print(f\"  Feedparser 'bozo' flag is set, indicating potential parsing problems. Exception: {feed_data.bozo_exception}\")\n",
    "                pass # Silencing for cleaner notebook output, uncomment for debugging\n",
    "\n",
    "        # Step 3: Process each entry from the parsed feed\n",
    "        for entry in entries_to_process:\n",
    "            title = entry.get('title')\n",
    "            url = entry.get('link') # The direct link to the article\n",
    "            \n",
    "            # Extract and parse publication date from the feed entry\n",
    "            pub_date_parsed_struct = entry.get('published_parsed') or entry.get('updated_parsed')\n",
    "            publication_date_from_feed = parse_datetime(pub_date_parsed_struct) if pub_date_parsed_struct else None\n",
    "\n",
    "            if not (title and url):\n",
    "                # print(f\"Skipping entry from '{source_name}' due to missing title or URL.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Fetch full article content using the extracted URL\n",
    "            # print(f\"  Fetching content for: '{title[:50]}...' from {url}\")\n",
    "            full_content, publication_date_from_article = fetch_article_content(url)\n",
    "            \n",
    "            # Determine the most reliable publication date\n",
    "            final_publication_date = publication_date_from_feed\n",
    "            if not final_publication_date and publication_date_from_article:\n",
    "                final_publication_date = publication_date_from_article\n",
    "            elif not final_publication_date: # If both are None, fallback to current time\n",
    "                final_publication_date = datetime.now(timezone.utc)\n",
    "\n",
    "            # Store the article data, using a placeholder if full content extraction failed\n",
    "            content_to_store = full_content if full_content and full_content.strip() else f\"Content not retrievable - {title}\"\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'publication_date': final_publication_date,\n",
    "                'full_content': content_to_store,\n",
    "                'source_name': source_name\n",
    "            })\n",
    "            \n",
    "            time.sleep(0.2) # Polite delay between fetching full articles to avoid overwhelming servers\n",
    "\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Network Error: Could not fetch RSS feed '{rss_url}'. Reason: {req_err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RSS scraping for '{source_name}' ({rss_url}): {e}\")\n",
    "        # import traceback # Uncomment for detailed error traceback during debugging\n",
    "        # traceback.print_exc()\n",
    "        \n",
    "    # Final check and reporting for this source\n",
    "    if feed_data and feed_data.entries and not articles and entries_to_process:\n",
    "        # This condition means entries were found in the feed, but none were successfully processed \n",
    "        # (e.g., fetch_article_content failed for all of them).\n",
    "        print(f\"Info: Found {len(feed_data.entries)} entries in '{source_name}', but article content extraction might have failed for all processed entries.\")\n",
    "    elif not articles:\n",
    "        # This means either no entries were found in the feed initially, or an early error occurred.\n",
    "        # print(f\"Info: No articles were successfully processed from '{source_name}'.\")\n",
    "        if feed_data is None:\n",
    "            # print(\"  (Reason: Feed data could not be fetched or parsed, likely due to a network error or malformed feed).\")\n",
    "            pass\n",
    "        elif not feed_data.entries:\n",
    "            # print(\"  (Reason: The RSS feed was parsed but contained no entries).\")\n",
    "            pass\n",
    "\n",
    "    return articles\n",
    "\n",
    "print(\"'scrape_rss_feed' function has been defined.\")\n",
    "\n",
    "# --- Test Execution for 'scrape_rss_feed' ---\n",
    "# This section demonstrates how to call the function and view its output.\n",
    "# Ensure NEWS_RSS_FEEDS is defined (usually in Cell 1 of this notebook).\n",
    "print(\"\\n--- Testing 'scrape_rss_feed' with configured RSS feeds: ---\")\n",
    "\n",
    "test_results_rss = []\n",
    "if NEWS_RSS_FEEDS:\n",
    "    for feed_name, feed_url in NEWS_RSS_FEEDS.items():\n",
    "        print(f\"\\nAttempting to process feed: {feed_name} ({feed_url})\")\n",
    "        scraped_articles_from_feed = scrape_rss_feed(feed_name, feed_url)\n",
    "        if scraped_articles_from_feed:\n",
    "            print(f\"  Successfully processed {len(scraped_articles_from_feed)} entries from '{feed_name}'.\")\n",
    "            test_results_rss.extend(scraped_articles_from_feed) # Add to combined list for display\n",
    "            # Display a sample from this specific feed\n",
    "            display(pd.DataFrame(scraped_articles_from_feed).head(2)[['title', 'publication_date', 'source_name']])\n",
    "        else:\n",
    "            print(f\"  No articles were processed from '{feed_name}'.\")\n",
    "else:\n",
    "    print(\"No RSS feeds are configured in NEWS_RSS_FEEDS for testing.\")\n",
    "\n",
    "# Display a combined sample if multiple feeds were processed\n",
    "if test_results_rss:\n",
    "    print(\"\\n--- Combined RSS Scraping Test Summary (Sample) ---\")\n",
    "    all_rss_df = pd.DataFrame(test_results_rss)\n",
    "    display(all_rss_df.head()[['title', 'publication_date', 'source_name']])\n",
    "    print(f\"Total articles processed from all tested RSS feeds: {len(all_rss_df)}\")\n",
    "else:\n",
    "    print(\"\\nNo articles were processed from any RSS feeds during this test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f8e1ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'scrape_reddit_forum' function has been defined.\n",
      "\n",
      "--- Testing 'scrape_reddit_forum' with configured subreddits: ---\n",
      "\n",
      "Attempting to process subreddit: r/RenewableEnergy (using key: 'RenewableEnergySub')\n",
      "  Successfully processed 5 posts from r/RenewableEnergy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solar shines as Germany's top electricity sour...</td>\n",
       "      <td>2025-05-23 04:21:26+00:00</td>\n",
       "      <td>RenewableEnergySub r/RenewableEnergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House GOP moves to slash renewable energy tax ...</td>\n",
       "      <td>2025-05-22 20:01:50+00:00</td>\n",
       "      <td>RenewableEnergySub r/RenewableEnergy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Solar shines as Germany's top electricity sour...   \n",
       "1  House GOP moves to slash renewable energy tax ...   \n",
       "\n",
       "           publication_date                           source_name  \n",
       "0 2025-05-23 04:21:26+00:00  RenewableEnergySub r/RenewableEnergy  \n",
       "1 2025-05-22 20:01:50+00:00  RenewableEnergySub r/RenewableEnergy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined Reddit Scraping Test Summary (Sample) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solar shines as Germany's top electricity sour...</td>\n",
       "      <td>2025-05-23 04:21:26+00:00</td>\n",
       "      <td>RenewableEnergySub r/RenewableEnergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House GOP moves to slash renewable energy tax ...</td>\n",
       "      <td>2025-05-22 20:01:50+00:00</td>\n",
       "      <td>RenewableEnergySub r/RenewableEnergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama enacts ‘all-of-the-above’ energy plan</td>\n",
       "      <td>2025-05-22 18:39:16+00:00</td>\n",
       "      <td>RenewableEnergySub r/RenewableEnergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fluence just took a big step to make grid batt...</td>\n",
       "      <td>2025-05-22 12:01:57+00:00</td>\n",
       "      <td>RenewableEnergySub r/RenewableEnergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New York awards contracts for 26 large-scale r...</td>\n",
       "      <td>2025-05-22 10:59:12+00:00</td>\n",
       "      <td>RenewableEnergySub r/RenewableEnergy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Solar shines as Germany's top electricity sour...   \n",
       "1  House GOP moves to slash renewable energy tax ...   \n",
       "2      Alabama enacts ‘all-of-the-above’ energy plan   \n",
       "3  Fluence just took a big step to make grid batt...   \n",
       "4  New York awards contracts for 26 large-scale r...   \n",
       "\n",
       "           publication_date                           source_name  \n",
       "0 2025-05-23 04:21:26+00:00  RenewableEnergySub r/RenewableEnergy  \n",
       "1 2025-05-22 20:01:50+00:00  RenewableEnergySub r/RenewableEnergy  \n",
       "2 2025-05-22 18:39:16+00:00  RenewableEnergySub r/RenewableEnergy  \n",
       "3 2025-05-22 12:01:57+00:00  RenewableEnergySub r/RenewableEnergy  \n",
       "4 2025-05-22 10:59:12+00:00  RenewableEnergySub r/RenewableEnergy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts processed from all tested subreddits: 5\n"
     ]
    }
   ],
   "source": [
    "# Assumes fetch_article_content and parse_datetime are defined in previous cells\n",
    "# Assumes REDDIT_POST_LIMIT and REDDIT_SUBREDDITS are defined in Cell 1 of this notebook\n",
    "\n",
    "def scrape_reddit_forum(source_name_prefix, subreddit_name, limit=5):\n",
    "    \"\"\"\n",
    "    Scrapes recent posts from a specified subreddit using its public JSON endpoint.\n",
    "    For posts that are links to external articles, it attempts to fetch the full\n",
    "    article content using the 'fetch_article_content' function.\n",
    "\n",
    "    Args:\n",
    "        source_name_prefix (str): A prefix for the source name (e.g., 'RedditTopic').\n",
    "        subreddit_name (str): The name of the subreddit to scrape (e.g., 'RenewableEnergy').\n",
    "        limit (int): The maximum number of recent posts to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a scraped post/article.\n",
    "              Returns an empty list if scraping fails or no posts are processed.\n",
    "    \"\"\"\n",
    "    # print(f\"Attempting to scrape Reddit: r/{subreddit_name}, limit={limit}\") # Kept for verbosity if desired\n",
    "    articles = []\n",
    "    \n",
    "    # It's good practice to set a custom User-Agent for web scraping to be identifiable\n",
    "    # and to avoid being blocked as a generic bot. Replace 'yourcontact@example.com'\n",
    "    # with your actual contact info if you were running this extensively.\n",
    "    headers = {\n",
    "        'User-agent': f'Mozilla/5.0 (compatible; {source_name_prefix}_TrendAnalyzerBot/0.2; +yourcontact@example.com)'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Construct the URL for accessing the subreddit's 'new' posts via its public JSON API.\n",
    "        url = f\"https://www.reddit.com/r/{subreddit_name}/new.json?limit={limit}\"\n",
    "        response = requests.get(url, headers=headers, timeout=15) # Timeout for the request\n",
    "        response.raise_for_status() # Will raise an HTTPError for bad status codes (4xx or 5xx)\n",
    "        \n",
    "        data = response.json() # Parse the JSON response\n",
    "\n",
    "        # Validate the structure of the received JSON data\n",
    "        if 'data' not in data or 'children' not in data['data']:\n",
    "            print(f\"  Error: Unexpected JSON structure from r/{subreddit_name}. Missing 'data' or 'children' field.\")\n",
    "            return articles # Return empty list if structure is not as expected\n",
    "\n",
    "        posts_to_process = data['data']['children']\n",
    "        # print(f\"Found {len(posts_to_process)} posts in r/{subreddit_name} JSON response to process.\")\n",
    "\n",
    "        for post in posts_to_process:\n",
    "            post_data = post['data']\n",
    "            title = post_data.get('title')\n",
    "            permalink = post_data.get('permalink')\n",
    "            \n",
    "            if not (title and permalink):\n",
    "                # print(f\"  Skipping post from r/{subreddit_name} due to missing title or permalink (ID: {post_data.get('id')}).\")\n",
    "                continue\n",
    "                \n",
    "            full_url = f\"https://www.reddit.com{permalink}\" # URL of the Reddit post itself\n",
    "            created_utc = post_data.get('created_utc')    # Timestamp of post creation\n",
    "            publication_date = parse_datetime(created_utc) # Standardize to datetime object\n",
    "            \n",
    "            content = \"\"\n",
    "            source_display_name = f\"{source_name_prefix} r/{subreddit_name}\" # Consistent source naming\n",
    "\n",
    "            # Differentiate between self-posts (text directly on Reddit) and link posts\n",
    "            if post_data.get('is_self', False): # Check if it's a self-post\n",
    "                content = post_data.get('selftext', '')\n",
    "                # print(f\"  Processing self-post from r/{subreddit_name}: '{title[:50]}...'\")\n",
    "            elif 'url_overridden_by_dest' in post_data: # Check if it's a link post\n",
    "                external_url = post_data['url_overridden_by_dest']\n",
    "                # print(f\"  Processing link post from r/{subreddit_name}: '{title[:50]}...' to {external_url[:50]}\")\n",
    "                fetched_content, _ = fetch_article_content(external_url) # Attempt to get external article content\n",
    "                if fetched_content and fetched_content.strip():\n",
    "                    content = fetched_content\n",
    "                else:\n",
    "                    # print(f\"    Could not fetch external content for {external_url}, using post title as fallback content.\")\n",
    "                    content = title # Fallback if external content fetching fails or is empty\n",
    "            else: # Other post types (e.g., direct image/video links not pointing to an article)\n",
    "                # print(f\"  Processing other post type from r/{subreddit_name} (e.g., direct media): '{title[:50]}...'\")\n",
    "                content = title # Fallback to using post title as content\n",
    "\n",
    "            # Add to list, using a placeholder if content is still empty after attempts\n",
    "            content_to_store = content if content and content.strip() else f\"Content not retrievable or empty - {title}\"\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'url': full_url, \n",
    "                'publication_date': publication_date,\n",
    "                'full_content': content_to_store,\n",
    "                'source_name': source_display_name\n",
    "            })\n",
    "            \n",
    "            time.sleep(0.5) # Polite delay to respect Reddit's servers when accessing public JSON\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP Error encountered while scraping Reddit r/{subreddit_name}: {http_err}\")\n",
    "        # It can be helpful to see the response text if an HTTP error occurs (e.g., 403 Forbidden, 404 Not Found)\n",
    "        # Be cautious with printing response.text if it might be very large or contain sensitive info in a real app.\n",
    "        # if hasattr(http_err, 'response') and http_err.response is not None:\n",
    "            # print(f\"  Response content sample: {http_err.response.text[:500]}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Network Error (requests): Could not scrape Reddit r/{subreddit_name}. Reason: {req_err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Reddit scraping for r/{subreddit_name}: {e}\")\n",
    "        # import traceback # Uncomment for detailed traceback during debugging\n",
    "        # traceback.print_exc()\n",
    "        \n",
    "    # Final reporting for this subreddit scrape attempt\n",
    "    # Using 'posts_to_process' which is defined inside the try block.\n",
    "    # Initialize it before the try block for safer access in this final reporting.\n",
    "    num_posts_found_in_json = len(data['data']['children']) if 'data' in locals() and data and 'data' in data and 'children' in data['data'] else 0\n",
    "    \n",
    "    if num_posts_found_in_json > 0 and not articles:\n",
    "        print(f\"Info: Found {num_posts_found_in_json} posts in r/{subreddit_name} JSON, but no articles were successfully processed (content fetching might have failed for all).\")\n",
    "    elif not articles:\n",
    "         print(f\"Info: No articles were successfully processed from r/{subreddit_name}.\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "print(\"'scrape_reddit_forum' function has been defined.\")\n",
    "\n",
    "# --- Test Execution for 'scrape_reddit_forum' ---\n",
    "# This section demonstrates how to call the function and view its output.\n",
    "# Ensure REDDIT_SUBREDDITS and REDDIT_POST_LIMIT are defined (usually in Cell 1).\n",
    "print(\"\\n--- Testing 'scrape_reddit_forum' with configured subreddits: ---\")\n",
    "\n",
    "test_results_reddit = []\n",
    "if REDDIT_SUBREDDITS:\n",
    "    for key, sub_name in REDDIT_SUBREDDITS.items(): # key is e.g., \"RenewableEnergySub\", sub_name is \"RenewableEnergy\"\n",
    "        print(f\"\\nAttempting to process subreddit: r/{sub_name} (using key: '{key}')\")\n",
    "        scraped_posts_from_sub = scrape_reddit_forum(key, sub_name, limit=REDDIT_POST_LIMIT)\n",
    "        if scraped_posts_from_sub:\n",
    "            print(f\"  Successfully processed {len(scraped_posts_from_sub)} posts from r/{sub_name}.\")\n",
    "            test_results_reddit.extend(scraped_posts_from_sub)\n",
    "             # Display a sample from this specific subreddit\n",
    "            display(pd.DataFrame(scraped_posts_from_sub).head(2)[['title', 'publication_date', 'source_name']])\n",
    "        else:\n",
    "            print(f\"  No posts were processed from r/{sub_name}.\")\n",
    "else:\n",
    "    print(\"No subreddits are configured in REDDIT_SUBREDDITS for testing.\")\n",
    "\n",
    "# Display a combined sample if multiple subreddits were processed\n",
    "if test_results_reddit:\n",
    "    print(\"\\n--- Combined Reddit Scraping Test Summary (Sample) ---\")\n",
    "    all_reddit_df = pd.DataFrame(test_results_reddit)\n",
    "    display(all_reddit_df.head()[['title', 'publication_date', 'source_name']])\n",
    "    print(f\"Total posts processed from all tested subreddits: {len(all_reddit_df)}\")\n",
    "else:\n",
    "    print(\"\\nNo posts were processed from any subreddits during this test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "993fab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_scrapers_notebook function defined.\n"
     ]
    }
   ],
   "source": [
    "def run_scrapers_notebook():\n",
    "    \"\"\"\n",
    "    Runs all configured scrapers (RSS and Reddit) and aggregates their results.\n",
    "    Returns a Pandas DataFrame of all scraped articles.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    \n",
    "    # Scrape RSS Feeds\n",
    "    if NEWS_RSS_FEEDS:\n",
    "        print(\"\\n--- Scraping RSS Feeds ---\")\n",
    "        for name, url in NEWS_RSS_FEEDS.items():\n",
    "            print(f\"Processing RSS: {name}...\")\n",
    "            rss_results = scrape_rss_feed(name, url) # This function already prints its own progress\n",
    "            if rss_results:\n",
    "                all_articles.extend(rss_results)\n",
    "            print(f\"Finished processing RSS: {name}. Found {len(rss_results)} articles.\")\n",
    "    else:\n",
    "        print(\"No RSS feeds configured to scrape.\")\n",
    "\n",
    "    # Scrape Reddit Subreddits\n",
    "    if REDDIT_SUBREDDITS:\n",
    "        print(\"\\n--- Scraping Reddit Subreddits ---\")\n",
    "        for key, sub_name in REDDIT_SUBREDDITS.items(): # key is like \"RenewableEnergySub\", sub_name is \"RenewableEnergy\"\n",
    "            print(f\"Processing Reddit: r/{sub_name}...\")\n",
    "            reddit_results = scrape_reddit_forum(key, sub_name, limit=REDDIT_POST_LIMIT) # Uses key as source_name_prefix\n",
    "            if reddit_results:\n",
    "                all_articles.extend(reddit_results)\n",
    "            print(f\"Finished processing Reddit: r/{sub_name}. Found {len(reddit_results)} posts.\")\n",
    "    else:\n",
    "        print(\"No Subreddits configured to scrape.\")\n",
    "    \n",
    "    print(f\"\\n--- Total articles/posts gathered from all sources: {len(all_articles)} ---\")\n",
    "    \n",
    "    if all_articles:\n",
    "        # Convert to DataFrame for easier handling and viewing\n",
    "        df_all_articles = pd.DataFrame(all_articles)\n",
    "        # Standardize publication_date to datetime objects if not already (should be handled by individual scrapers)\n",
    "        if 'publication_date' in df_all_articles.columns:\n",
    "            df_all_articles['publication_date'] = pd.to_datetime(df_all_articles['publication_date'], errors='coerce', utc=True)\n",
    "        return df_all_articles\n",
    "    \n",
    "    return pd.DataFrame() # Return empty DataFrame if nothing scraped\n",
    "\n",
    "print(\"run_scrapers_notebook function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b53c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running all configured scrapers via run_scrapers_notebook()...\n",
      "This will use REDDIT_POST_LIMIT = 5 for all sources in this notebook context.\n",
      "\n",
      "--- Scraping RSS Feeds ---\n",
      "Processing RSS: Guardian Environment...\n",
      "Finished processing RSS: Guardian Environment. Found 5 articles.\n",
      "Processing RSS: Ars Technica...\n",
      "Finished processing RSS: Ars Technica. Found 5 articles.\n",
      "\n",
      "--- Scraping Reddit Subreddits ---\n",
      "Processing Reddit: r/RenewableEnergy...\n",
      "Finished processing Reddit: r/RenewableEnergy. Found 5 posts.\n",
      "\n",
      "--- Total articles/posts gathered from all sources: 15 ---\n",
      "\n",
      "Combined scraping process took 21.90 seconds.\n",
      "\n",
      "--- Sample of all scraped data (first 5 rows): ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>full_content</th>\n",
       "      <th>source_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revealed: three tonnes of uranium legally dump...</td>\n",
       "      <td>https://www.theguardian.com/environment/2025/m...</td>\n",
       "      <td>2025-05-22 08:30:33+00:00</td>\n",
       "      <td>The Environment Agency has allowed a firm to d...</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘Unprecedented’ marine heatwave hits waters ar...</td>\n",
       "      <td>https://www.theguardian.com/environment/2025/m...</td>\n",
       "      <td>2025-05-22 07:57:36+00:00</td>\n",
       "      <td>The sea off the coast of the UK and Ireland is...</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How an idealistic tree-planting project turned...</td>\n",
       "      <td>https://www.theguardian.com/environment/2025/m...</td>\n",
       "      <td>2025-05-21 23:30:49+00:00</td>\n",
       "      <td>For his entire life, John Lmakato has lived in...</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘Waste collection is green work’: how a pro-po...</td>\n",
       "      <td>https://www.theguardian.com/environment/2025/m...</td>\n",
       "      <td>2025-05-22 08:10:29+00:00</td>\n",
       "      <td>Three decades ago, Rajabai Sawant used to pick...</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump’s tax bill to cost 830,000 jobs and driv...</td>\n",
       "      <td>https://www.theguardian.com/environment/2025/m...</td>\n",
       "      <td>2025-05-22 05:30:09+00:00</td>\n",
       "      <td>A Republican push to dismantle clean energy in...</td>\n",
       "      <td>Guardian Environment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Revealed: three tonnes of uranium legally dump...   \n",
       "1  ‘Unprecedented’ marine heatwave hits waters ar...   \n",
       "2  How an idealistic tree-planting project turned...   \n",
       "3  ‘Waste collection is green work’: how a pro-po...   \n",
       "4  Trump’s tax bill to cost 830,000 jobs and driv...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.theguardian.com/environment/2025/m...   \n",
       "1  https://www.theguardian.com/environment/2025/m...   \n",
       "2  https://www.theguardian.com/environment/2025/m...   \n",
       "3  https://www.theguardian.com/environment/2025/m...   \n",
       "4  https://www.theguardian.com/environment/2025/m...   \n",
       "\n",
       "           publication_date  \\\n",
       "0 2025-05-22 08:30:33+00:00   \n",
       "1 2025-05-22 07:57:36+00:00   \n",
       "2 2025-05-21 23:30:49+00:00   \n",
       "3 2025-05-22 08:10:29+00:00   \n",
       "4 2025-05-22 05:30:09+00:00   \n",
       "\n",
       "                                        full_content           source_name  \n",
       "0  The Environment Agency has allowed a firm to d...  Guardian Environment  \n",
       "1  The sea off the coast of the UK and Ireland is...  Guardian Environment  \n",
       "2  For his entire life, John Lmakato has lived in...  Guardian Environment  \n",
       "3  Three decades ago, Rajabai Sawant used to pick...  Guardian Environment  \n",
       "4  A Republican push to dismantle clean energy in...  Guardian Environment  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataframe Info: ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   title             15 non-null     object             \n",
      " 1   url               15 non-null     object             \n",
      " 2   publication_date  15 non-null     datetime64[ns, UTC]\n",
      " 3   full_content      15 non-null     object             \n",
      " 4   source_name       15 non-null     object             \n",
      "dtypes: datetime64[ns, UTC](1), object(4)\n",
      "memory usage: 728.0+ bytes\n",
      "\n",
      "--- Value Counts for 'source_name': ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Guardian Environment                    5\n",
       "Ars Technica                            5\n",
       "RenewableEnergySub r/RenewableEnergy    5\n",
       "Name: source_name, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved all scraped data to 'scraped_articles_combined_notebook.csv'\n"
     ]
    }
   ],
   "source": [
    "print(\"Running all configured scrapers via run_scrapers_notebook()...\")\n",
    "print(f\"This will use REDDIT_POST_LIMIT = {REDDIT_POST_LIMIT} for all sources in this notebook context.\")\n",
    "\n",
    "# Before running, ensure your NEWS_RSS_FEEDS in Cell 1 contains valid and relevant URLs\n",
    "# that worked in your earlier tests.\n",
    "# If NEWS_RSS_FEEDS is empty or has non-working URLs, it will only scrape Reddit (if configured).\n",
    "\n",
    "start_time_combined = time.time()\n",
    "scraped_data_df = run_scrapers_notebook()\n",
    "end_time_combined = time.time()\n",
    "\n",
    "print(f\"\\nCombined scraping process took {end_time_combined - start_time_combined:.2f} seconds.\")\n",
    "\n",
    "if not scraped_data_df.empty:\n",
    "    print(\"\\n--- Sample of all scraped data (first 5 rows): ---\")\n",
    "    display(scraped_data_df.head())\n",
    "    \n",
    "    print(\"\\n--- Dataframe Info: ---\")\n",
    "    scraped_data_df.info()\n",
    "    \n",
    "    print(\"\\n--- Value Counts for 'source_name': ---\")\n",
    "    display(scraped_data_df['source_name'].value_counts())\n",
    "    \n",
    "    # Optional: Save all scraped data to a CSV for use in the next notebook\n",
    "    # This can be helpful so you don't have to re-scrape every time.\n",
    "    try:\n",
    "        scraped_data_df.to_csv(\"scraped_articles_combined_notebook.csv\", index=False)\n",
    "        print(\"\\nSuccessfully saved all scraped data to 'scraped_articles_combined_notebook.csv'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving to CSV: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo articles were scraped in the combined run from any source.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a7ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
